{"cells":[{"cell_type":"code","execution_count":3,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-05-28T17:23:12.956533Z","iopub.status.busy":"2024-05-28T17:23:12.956243Z","iopub.status.idle":"2024-05-28T17:23:12.965808Z","shell.execute_reply":"2024-05-28T17:23:12.964903Z","shell.execute_reply.started":"2024-05-28T17:23:12.956510Z"},"trusted":true},"outputs":[],"source":["import torchvision.transforms as transforms\n","from torch.utils.data import DataLoader, random_split\n","from torchvision import datasets\n","\n","class RSSCN7_DataLoader:\n","    def __init__(self, data_dir, batch_size=32, shuffle=False):\n","        self.data_dir = data_dir\n","        self.batch_size = batch_size\n","        self.shuffle = shuffle\n","\n","        self.transform = transforms.Compose([\n","            transforms.Resize((256, 256)),\n","            transforms.ToTensor(),\n","            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","        ])\n","\n","        self.dataset = datasets.ImageFolder(root=self.data_dir, transform=self.transform)\n","        self.train_dataset, self.test_dataset = self.split_dataset()\n","\n","    def split_dataset(self):\n","        train_size = int(0.8 * len(self.dataset))\n","        test_size = len(self.dataset) - train_size\n","\n","        train_dataset, test_dataset = random_split(self.dataset, [train_size, test_size])\n","        return train_dataset, test_dataset\n","\n","    def get_train_dataloader(self):\n","        return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=self.shuffle)\n","\n","    def get_test_dataloader(self):\n","        return DataLoader(self.test_dataset, batch_size=self.batch_size, shuffle=False)\n"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-05-28T17:23:15.626189Z","iopub.status.busy":"2024-05-28T17:23:15.625807Z","iopub.status.idle":"2024-05-28T17:23:15.634633Z","shell.execute_reply":"2024-05-28T17:23:15.633753Z","shell.execute_reply.started":"2024-05-28T17:23:15.626160Z"},"trusted":true},"outputs":[],"source":["from torch import nn\n","import torchvision.models as models\n","\n","class ResNet18(nn.Module):\n","    def __init__(self, num_classes=47):\n","        super(ResNet18, self).__init__()\n","        self.resnet18 = models.resnet18(pretrained=False)\n","        self.resnet18.fc = nn.Linear(self.resnet18.fc.in_features, num_classes)\n","\n","    def forward(self, x):\n","        return self.resnet18(x)"]},{"cell_type":"markdown","metadata":{},"source":["ImageNet, self paced"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-05-27T20:12:37.224132Z","iopub.status.busy":"2024-05-27T20:12:37.223591Z","iopub.status.idle":"2024-05-27T20:51:51.760802Z","shell.execute_reply":"2024-05-27T20:51:51.759768Z","shell.execute_reply.started":"2024-05-27T20:12:37.224093Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n","100%|██████████| 44.7M/44.7M [00:00<00:00, 143MB/s] \n"]},{"name":"stdout","output_type":"stream","text":["learing_rate =  0.0001\n","Epoch [1/100], Loss: 1.3892, Accuracy: 0.5223, Images: 224, Lambda: 0.10, Time: 29.14 seconds\n","Test Loss: 1.1494, Test Accuracy: 0.5571\n","learing_rate =  0.0001\n","Epoch [2/100], Loss: 0.6462, Accuracy: 0.7857, Images: 224, Lambda: 0.10, Time: 51.34 seconds\n","Test Loss: 0.6834, Test Accuracy: 0.7607\n","learing_rate =  1e-05\n","Epoch [3/100], Loss: 0.3638, Accuracy: 0.9152, Images: 224, Lambda: 0.10, Time: 71.57 seconds\n","Test Loss: 0.4862, Test Accuracy: 0.8214\n","learing_rate =  1e-05\n","Epoch [4/100], Loss: 0.3000, Accuracy: 0.9094, Images: 320, Lambda: 0.15, Time: 91.21 seconds\n","Test Loss: 0.3185, Test Accuracy: 0.8911\n","learing_rate =  1e-06\n","Epoch [5/100], Loss: 0.2142, Accuracy: 0.9375, Images: 448, Lambda: 0.20, Time: 111.75 seconds\n","Test Loss: 0.2383, Test Accuracy: 0.9196\n","learing_rate =  1e-06\n","Epoch [6/100], Loss: 0.1655, Accuracy: 0.9485, Images: 544, Lambda: 0.25, Time: 133.74 seconds\n","Test Loss: 0.1914, Test Accuracy: 0.9286\n","learing_rate =  1e-06\n","Epoch [7/100], Loss: 0.1742, Accuracy: 0.9539, Images: 672, Lambda: 0.30, Time: 155.07 seconds\n","Test Loss: 0.2015, Test Accuracy: 0.9286\n","learing_rate =  1e-07\n","Epoch [8/100], Loss: 0.1060, Accuracy: 0.9674, Images: 768, Lambda: 0.35, Time: 177.12 seconds\n","Test Loss: 0.1969, Test Accuracy: 0.9268\n","learing_rate =  1e-07\n","Epoch [9/100], Loss: 0.0892, Accuracy: 0.9745, Images: 864, Lambda: 0.40, Time: 199.46 seconds\n","Test Loss: 0.1522, Test Accuracy: 0.9446\n","learing_rate =  1e-07\n","Epoch [10/100], Loss: 0.1014, Accuracy: 0.9677, Images: 992, Lambda: 0.45, Time: 221.85 seconds\n","Test Loss: 0.1890, Test Accuracy: 0.9339\n","learing_rate =  1e-07\n","Epoch [11/100], Loss: 0.0915, Accuracy: 0.9743, Images: 1088, Lambda: 0.50, Time: 245.11 seconds\n","Test Loss: 0.1495, Test Accuracy: 0.9536\n","learing_rate =  1e-07\n","Epoch [12/100], Loss: 0.0768, Accuracy: 0.9786, Images: 1216, Lambda: 0.55, Time: 268.75 seconds\n","Test Loss: 0.1571, Test Accuracy: 0.9482\n","learing_rate =  1e-07\n","Epoch [13/100], Loss: 0.0859, Accuracy: 0.9688, Images: 1344, Lambda: 0.60, Time: 292.21 seconds\n","Test Loss: 0.1808, Test Accuracy: 0.9393\n","learing_rate =  1e-07\n","Epoch [14/100], Loss: 0.0422, Accuracy: 0.9888, Images: 1344, Lambda: 0.60, Time: 316.01 seconds\n","Test Loss: 0.1552, Test Accuracy: 0.9518\n","learing_rate =  1e-07\n","Epoch [15/100], Loss: 0.0195, Accuracy: 0.9965, Images: 1440, Lambda: 0.65, Time: 340.27 seconds\n","Test Loss: 0.1213, Test Accuracy: 0.9625\n","learing_rate =  1e-07\n","Epoch [16/100], Loss: 0.0234, Accuracy: 0.9951, Images: 1440, Lambda: 0.65, Time: 364.48 seconds\n","Test Loss: 0.1337, Test Accuracy: 0.9554\n","learing_rate =  1e-07\n","Epoch [17/100], Loss: 0.0222, Accuracy: 0.9955, Images: 1568, Lambda: 0.70, Time: 388.86 seconds\n","Test Loss: 0.2020, Test Accuracy: 0.9429\n","learing_rate =  1e-07\n","Epoch [18/100], Loss: 0.0274, Accuracy: 0.9943, Images: 1568, Lambda: 0.70, Time: 413.09 seconds\n","Test Loss: 0.1535, Test Accuracy: 0.9571\n","learing_rate =  1e-07\n","Epoch [19/100], Loss: 0.0169, Accuracy: 0.9964, Images: 1664, Lambda: 0.75, Time: 438.24 seconds\n","Test Loss: 0.1513, Test Accuracy: 0.9482\n","learing_rate =  1e-07\n","Epoch [20/100], Loss: 0.0193, Accuracy: 0.9952, Images: 1664, Lambda: 0.75, Time: 463.15 seconds\n","Test Loss: 0.1463, Test Accuracy: 0.9500\n","learing_rate =  1e-07\n","Epoch [21/100], Loss: 0.0198, Accuracy: 0.9950, Images: 1792, Lambda: 0.80, Time: 488.62 seconds\n","Test Loss: 0.1849, Test Accuracy: 0.9411\n","learing_rate =  1e-07\n","Epoch [22/100], Loss: 0.0137, Accuracy: 0.9978, Images: 1792, Lambda: 0.80, Time: 513.62 seconds\n","Test Loss: 0.1675, Test Accuracy: 0.9536\n","learing_rate =  1e-07\n","Epoch [23/100], Loss: 0.0175, Accuracy: 0.9942, Images: 1888, Lambda: 0.85, Time: 538.87 seconds\n","Test Loss: 0.2282, Test Accuracy: 0.9232\n","learing_rate =  1e-07\n","Epoch [24/100], Loss: 0.0256, Accuracy: 0.9931, Images: 1888, Lambda: 0.85, Time: 564.88 seconds\n","Test Loss: 0.1880, Test Accuracy: 0.9393\n","learing_rate =  1e-07\n","Epoch [25/100], Loss: 0.0148, Accuracy: 0.9975, Images: 2016, Lambda: 0.90, Time: 590.99 seconds\n","Test Loss: 0.1934, Test Accuracy: 0.9429\n","learing_rate =  1e-07\n","Epoch [26/100], Loss: 0.0069, Accuracy: 0.9995, Images: 2016, Lambda: 0.90, Time: 617.25 seconds\n","Test Loss: 0.1533, Test Accuracy: 0.9536\n","learing_rate =  1e-07\n","Epoch [27/100], Loss: 0.0211, Accuracy: 0.9957, Images: 2112, Lambda: 0.95, Time: 643.17 seconds\n","Test Loss: 0.1334, Test Accuracy: 0.9571\n","learing_rate =  1e-07\n","Epoch [28/100], Loss: 0.0054, Accuracy: 0.9995, Images: 2112, Lambda: 0.95, Time: 669.69 seconds\n","Test Loss: 0.1263, Test Accuracy: 0.9571\n","learing_rate =  1e-07\n","Epoch [29/100], Loss: 0.0211, Accuracy: 0.9973, Images: 2240, Lambda: 1.00, Time: 692.86 seconds\n","Test Loss: 0.1226, Test Accuracy: 0.9571\n","learing_rate =  1e-07\n","Epoch [30/100], Loss: 0.0515, Accuracy: 0.9848, Images: 2240, Lambda: 1.00, Time: 716.08 seconds\n","Test Loss: 0.2252, Test Accuracy: 0.9304\n","learing_rate =  1e-07\n","Epoch [31/100], Loss: 0.0247, Accuracy: 0.9938, Images: 2240, Lambda: 1.00, Time: 739.75 seconds\n","Test Loss: 0.1979, Test Accuracy: 0.9411\n","learing_rate =  1e-07\n","Epoch [32/100], Loss: 0.0248, Accuracy: 0.9911, Images: 2240, Lambda: 1.00, Time: 762.86 seconds\n","Test Loss: 0.1740, Test Accuracy: 0.9518\n","learing_rate =  1e-07\n","Epoch [33/100], Loss: 0.0341, Accuracy: 0.9884, Images: 2240, Lambda: 1.00, Time: 786.47 seconds\n","Test Loss: 0.1919, Test Accuracy: 0.9357\n","learing_rate =  1e-07\n","Epoch [34/100], Loss: 0.0329, Accuracy: 0.9862, Images: 2240, Lambda: 1.00, Time: 810.03 seconds\n","Test Loss: 0.3081, Test Accuracy: 0.9143\n","learing_rate =  1e-07\n","Epoch [35/100], Loss: 0.0261, Accuracy: 0.9920, Images: 2240, Lambda: 1.00, Time: 833.43 seconds\n","Test Loss: 0.1785, Test Accuracy: 0.9464\n","learing_rate =  1e-07\n","Epoch [36/100], Loss: 0.0138, Accuracy: 0.9978, Images: 2240, Lambda: 1.00, Time: 857.04 seconds\n","Test Loss: 0.1300, Test Accuracy: 0.9536\n","learing_rate =  1e-07\n","Epoch [37/100], Loss: 0.0042, Accuracy: 0.9996, Images: 2240, Lambda: 1.00, Time: 880.26 seconds\n","Test Loss: 0.1157, Test Accuracy: 0.9643\n","learing_rate =  1e-07\n","Epoch [38/100], Loss: 0.0096, Accuracy: 0.9973, Images: 2240, Lambda: 1.00, Time: 903.60 seconds\n","Test Loss: 0.1382, Test Accuracy: 0.9571\n","learing_rate =  1e-07\n","Epoch [39/100], Loss: 0.0075, Accuracy: 0.9982, Images: 2240, Lambda: 1.00, Time: 926.91 seconds\n","Test Loss: 0.1231, Test Accuracy: 0.9536\n","learing_rate =  1e-07\n","Epoch [40/100], Loss: 0.0016, Accuracy: 1.0000, Images: 2240, Lambda: 1.00, Time: 950.31 seconds\n","Test Loss: 0.1334, Test Accuracy: 0.9607\n","learing_rate =  1e-07\n","Epoch [41/100], Loss: 0.0017, Accuracy: 1.0000, Images: 2240, Lambda: 1.00, Time: 973.77 seconds\n","Test Loss: 0.1352, Test Accuracy: 0.9554\n","learing_rate =  1e-07\n","Epoch [42/100], Loss: 0.0007, Accuracy: 1.0000, Images: 2240, Lambda: 1.00, Time: 997.31 seconds\n","Test Loss: 0.1260, Test Accuracy: 0.9589\n","learing_rate =  1e-07\n","Epoch [43/100], Loss: 0.0009, Accuracy: 1.0000, Images: 2240, Lambda: 1.00, Time: 1020.69 seconds\n","Test Loss: 0.1346, Test Accuracy: 0.9679\n","learing_rate =  1e-07\n","Epoch [44/100], Loss: 0.0009, Accuracy: 1.0000, Images: 2240, Lambda: 1.00, Time: 1044.58 seconds\n","Test Loss: 0.1203, Test Accuracy: 0.9607\n","learing_rate =  1e-07\n","Epoch [45/100], Loss: 0.0034, Accuracy: 0.9987, Images: 2240, Lambda: 1.00, Time: 1067.85 seconds\n","Test Loss: 0.1326, Test Accuracy: 0.9554\n","learing_rate =  1e-07\n","Epoch [46/100], Loss: 0.0023, Accuracy: 1.0000, Images: 2240, Lambda: 1.00, Time: 1091.33 seconds\n","Test Loss: 0.1313, Test Accuracy: 0.9589\n","learing_rate =  1e-07\n","Epoch [47/100], Loss: 0.0037, Accuracy: 0.9991, Images: 2240, Lambda: 1.00, Time: 1114.53 seconds\n","Test Loss: 0.1535, Test Accuracy: 0.9536\n","learing_rate =  1e-07\n","Epoch [48/100], Loss: 0.0121, Accuracy: 0.9969, Images: 2240, Lambda: 1.00, Time: 1137.96 seconds\n","Test Loss: 0.1533, Test Accuracy: 0.9446\n","learing_rate =  1e-07\n","Epoch [49/100], Loss: 0.0079, Accuracy: 0.9978, Images: 2240, Lambda: 1.00, Time: 1161.60 seconds\n","Test Loss: 0.1925, Test Accuracy: 0.9429\n","learing_rate =  1e-07\n","Epoch [50/100], Loss: 0.0058, Accuracy: 0.9987, Images: 2240, Lambda: 1.00, Time: 1184.73 seconds\n","Test Loss: 0.1493, Test Accuracy: 0.9536\n","learing_rate =  1e-07\n","Epoch [51/100], Loss: 0.0032, Accuracy: 0.9996, Images: 2240, Lambda: 1.00, Time: 1208.25 seconds\n","Test Loss: 0.1576, Test Accuracy: 0.9518\n","learing_rate =  1e-07\n","Epoch [52/100], Loss: 0.0015, Accuracy: 1.0000, Images: 2240, Lambda: 1.00, Time: 1231.49 seconds\n","Test Loss: 0.1468, Test Accuracy: 0.9536\n","learing_rate =  1e-07\n","Epoch [53/100], Loss: 0.0040, Accuracy: 0.9991, Images: 2240, Lambda: 1.00, Time: 1254.60 seconds\n","Test Loss: 0.2876, Test Accuracy: 0.9232\n","learing_rate =  1e-07\n","Epoch [54/100], Loss: 0.0214, Accuracy: 0.9920, Images: 2240, Lambda: 1.00, Time: 1278.11 seconds\n","Test Loss: 0.3487, Test Accuracy: 0.9232\n","learing_rate =  1e-07\n","Epoch [55/100], Loss: 0.0269, Accuracy: 0.9924, Images: 2240, Lambda: 1.00, Time: 1301.17 seconds\n","Test Loss: 0.2314, Test Accuracy: 0.9375\n","learing_rate =  1e-07\n","Epoch [56/100], Loss: 0.0213, Accuracy: 0.9933, Images: 2240, Lambda: 1.00, Time: 1324.24 seconds\n","Test Loss: 0.2804, Test Accuracy: 0.9196\n","learing_rate =  1e-07\n","Epoch [57/100], Loss: 0.0287, Accuracy: 0.9897, Images: 2240, Lambda: 1.00, Time: 1347.74 seconds\n","Test Loss: 0.2864, Test Accuracy: 0.9196\n","learing_rate =  1e-07\n","Epoch [58/100], Loss: 0.0390, Accuracy: 0.9866, Images: 2240, Lambda: 1.00, Time: 1370.77 seconds\n","Test Loss: 0.2209, Test Accuracy: 0.9446\n","learing_rate =  1e-07\n","Epoch [59/100], Loss: 0.0232, Accuracy: 0.9929, Images: 2240, Lambda: 1.00, Time: 1394.52 seconds\n","Test Loss: 0.1724, Test Accuracy: 0.9464\n","learing_rate =  1e-07\n","Epoch [60/100], Loss: 0.0050, Accuracy: 0.9987, Images: 2240, Lambda: 1.00, Time: 1417.48 seconds\n","Test Loss: 0.1861, Test Accuracy: 0.9500\n","learing_rate =  1e-07\n","Epoch [61/100], Loss: 0.0024, Accuracy: 1.0000, Images: 2240, Lambda: 1.00, Time: 1440.65 seconds\n","Test Loss: 0.1510, Test Accuracy: 0.9571\n","learing_rate =  1e-07\n","Epoch [62/100], Loss: 0.0013, Accuracy: 0.9996, Images: 2240, Lambda: 1.00, Time: 1464.11 seconds\n","Test Loss: 0.1439, Test Accuracy: 0.9536\n","learing_rate =  1e-07\n","Epoch [63/100], Loss: 0.0012, Accuracy: 1.0000, Images: 2240, Lambda: 1.00, Time: 1487.34 seconds\n","Test Loss: 0.1565, Test Accuracy: 0.9571\n","learing_rate =  1e-07\n","Epoch [64/100], Loss: 0.0018, Accuracy: 1.0000, Images: 2240, Lambda: 1.00, Time: 1510.68 seconds\n","Test Loss: 0.1509, Test Accuracy: 0.9589\n","learing_rate =  1e-07\n","Epoch [65/100], Loss: 0.0007, Accuracy: 1.0000, Images: 2240, Lambda: 1.00, Time: 1534.08 seconds\n","Test Loss: 0.1387, Test Accuracy: 0.9589\n","learing_rate =  1e-07\n","Epoch [66/100], Loss: 0.0011, Accuracy: 1.0000, Images: 2240, Lambda: 1.00, Time: 1557.94 seconds\n","Test Loss: 0.1418, Test Accuracy: 0.9625\n","learing_rate =  1e-07\n","Epoch [67/100], Loss: 0.0033, Accuracy: 0.9987, Images: 2240, Lambda: 1.00, Time: 1581.27 seconds\n","Test Loss: 0.1922, Test Accuracy: 0.9518\n","learing_rate =  1e-07\n","Epoch [68/100], Loss: 0.0103, Accuracy: 0.9969, Images: 2240, Lambda: 1.00, Time: 1604.29 seconds\n","Test Loss: 0.2091, Test Accuracy: 0.9482\n","learing_rate =  1e-07\n","Epoch [69/100], Loss: 0.0052, Accuracy: 0.9982, Images: 2240, Lambda: 1.00, Time: 1627.92 seconds\n","Test Loss: 0.1832, Test Accuracy: 0.9554\n","learing_rate =  1e-07\n","Epoch [70/100], Loss: 0.0140, Accuracy: 0.9951, Images: 2240, Lambda: 1.00, Time: 1651.33 seconds\n","Test Loss: 0.3392, Test Accuracy: 0.9304\n","learing_rate =  1e-07\n","Epoch [71/100], Loss: 0.0216, Accuracy: 0.9929, Images: 2240, Lambda: 1.00, Time: 1674.61 seconds\n","Test Loss: 0.3128, Test Accuracy: 0.9196\n","learing_rate =  1e-07\n","Epoch [72/100], Loss: 0.0176, Accuracy: 0.9942, Images: 2240, Lambda: 1.00, Time: 1697.86 seconds\n","Test Loss: 0.1938, Test Accuracy: 0.9482\n","learing_rate =  1e-07\n","Epoch [73/100], Loss: 0.0166, Accuracy: 0.9955, Images: 2240, Lambda: 1.00, Time: 1721.05 seconds\n","Test Loss: 0.1677, Test Accuracy: 0.9500\n","learing_rate =  1e-07\n","Epoch [74/100], Loss: 0.0074, Accuracy: 0.9973, Images: 2240, Lambda: 1.00, Time: 1744.33 seconds\n","Test Loss: 0.0997, Test Accuracy: 0.9661\n","learing_rate =  1e-07\n","Epoch [75/100], Loss: 0.0036, Accuracy: 0.9987, Images: 2240, Lambda: 1.00, Time: 1767.73 seconds\n","Test Loss: 0.1251, Test Accuracy: 0.9571\n","learing_rate =  1e-07\n","Epoch [76/100], Loss: 0.0084, Accuracy: 0.9969, Images: 2240, Lambda: 1.00, Time: 1790.80 seconds\n","Test Loss: 0.1555, Test Accuracy: 0.9518\n","learing_rate =  1e-07\n","Epoch [77/100], Loss: 0.0242, Accuracy: 0.9915, Images: 2240, Lambda: 1.00, Time: 1814.11 seconds\n","Test Loss: 0.1872, Test Accuracy: 0.9429\n","learing_rate =  1e-07\n","Epoch [78/100], Loss: 0.0091, Accuracy: 0.9991, Images: 2240, Lambda: 1.00, Time: 1837.40 seconds\n","Test Loss: 0.1540, Test Accuracy: 0.9429\n","learing_rate =  1e-07\n","Epoch [79/100], Loss: 0.0080, Accuracy: 0.9982, Images: 2240, Lambda: 1.00, Time: 1860.81 seconds\n","Test Loss: 0.1530, Test Accuracy: 0.9536\n","learing_rate =  1e-07\n","Epoch [80/100], Loss: 0.0021, Accuracy: 1.0000, Images: 2240, Lambda: 1.00, Time: 1883.90 seconds\n","Test Loss: 0.1226, Test Accuracy: 0.9554\n","learing_rate =  1e-07\n","Epoch [81/100], Loss: 0.0015, Accuracy: 1.0000, Images: 2240, Lambda: 1.00, Time: 1907.28 seconds\n","Test Loss: 0.1317, Test Accuracy: 0.9589\n","learing_rate =  1e-07\n","Epoch [82/100], Loss: 0.0004, Accuracy: 1.0000, Images: 2240, Lambda: 1.00, Time: 1930.59 seconds\n","Test Loss: 0.1229, Test Accuracy: 0.9554\n","learing_rate =  1e-07\n","Epoch [83/100], Loss: 0.0008, Accuracy: 1.0000, Images: 2240, Lambda: 1.00, Time: 1953.68 seconds\n","Test Loss: 0.1139, Test Accuracy: 0.9607\n","learing_rate =  1e-07\n","Epoch [84/100], Loss: 0.0003, Accuracy: 1.0000, Images: 2240, Lambda: 1.00, Time: 1977.04 seconds\n","Test Loss: 0.1094, Test Accuracy: 0.9571\n","learing_rate =  1e-07\n","Epoch [85/100], Loss: 0.0006, Accuracy: 1.0000, Images: 2240, Lambda: 1.00, Time: 2000.55 seconds\n","Test Loss: 0.1059, Test Accuracy: 0.9554\n","learing_rate =  1e-07\n","Epoch [86/100], Loss: 0.0004, Accuracy: 1.0000, Images: 2240, Lambda: 1.00, Time: 2023.62 seconds\n","Test Loss: 0.1098, Test Accuracy: 0.9589\n","learing_rate =  1e-07\n","Epoch [87/100], Loss: 0.0002, Accuracy: 1.0000, Images: 2240, Lambda: 1.00, Time: 2047.15 seconds\n","Test Loss: 0.1078, Test Accuracy: 0.9607\n","learing_rate =  1e-07\n","Epoch [88/100], Loss: 0.0003, Accuracy: 1.0000, Images: 2240, Lambda: 1.00, Time: 2070.62 seconds\n","Test Loss: 0.1085, Test Accuracy: 0.9607\n","learing_rate =  1e-07\n","Epoch [89/100], Loss: 0.0002, Accuracy: 1.0000, Images: 2240, Lambda: 1.00, Time: 2093.89 seconds\n","Test Loss: 0.1046, Test Accuracy: 0.9625\n","learing_rate =  1e-07\n","Epoch [90/100], Loss: 0.0005, Accuracy: 1.0000, Images: 2240, Lambda: 1.00, Time: 2117.69 seconds\n","Test Loss: 0.1043, Test Accuracy: 0.9625\n","learing_rate =  1e-07\n","Epoch [91/100], Loss: 0.0003, Accuracy: 1.0000, Images: 2240, Lambda: 1.00, Time: 2140.88 seconds\n","Test Loss: 0.1193, Test Accuracy: 0.9589\n","learing_rate =  1e-07\n","Epoch [92/100], Loss: 0.0002, Accuracy: 1.0000, Images: 2240, Lambda: 1.00, Time: 2164.67 seconds\n","Test Loss: 0.1138, Test Accuracy: 0.9589\n","learing_rate =  1e-07\n","Epoch [93/100], Loss: 0.0008, Accuracy: 1.0000, Images: 2240, Lambda: 1.00, Time: 2187.85 seconds\n","Test Loss: 0.1136, Test Accuracy: 0.9625\n","learing_rate =  1e-07\n","Epoch [94/100], Loss: 0.0002, Accuracy: 1.0000, Images: 2240, Lambda: 1.00, Time: 2210.90 seconds\n","Test Loss: 0.1146, Test Accuracy: 0.9589\n","learing_rate =  1e-07\n","Epoch [95/100], Loss: 0.0004, Accuracy: 1.0000, Images: 2240, Lambda: 1.00, Time: 2234.32 seconds\n","Test Loss: 0.1094, Test Accuracy: 0.9607\n","learing_rate =  1e-07\n","Epoch [96/100], Loss: 0.0002, Accuracy: 1.0000, Images: 2240, Lambda: 1.00, Time: 2257.44 seconds\n","Test Loss: 0.1021, Test Accuracy: 0.9589\n","learing_rate =  1e-07\n","Epoch [97/100], Loss: 0.0002, Accuracy: 1.0000, Images: 2240, Lambda: 1.00, Time: 2280.64 seconds\n","Test Loss: 0.1063, Test Accuracy: 0.9625\n","learing_rate =  1e-07\n","Epoch [98/100], Loss: 0.0003, Accuracy: 1.0000, Images: 2240, Lambda: 1.00, Time: 2304.03 seconds\n","Test Loss: 0.1059, Test Accuracy: 0.9589\n","learing_rate =  1e-07\n","Epoch [99/100], Loss: 0.0001, Accuracy: 1.0000, Images: 2240, Lambda: 1.00, Time: 2326.98 seconds\n","Test Loss: 0.1095, Test Accuracy: 0.9643\n","learing_rate =  1e-07\n","Epoch [100/100], Loss: 0.0001, Accuracy: 1.0000, Images: 2240, Lambda: 1.00, Time: 2350.80 seconds\n","Test Loss: 0.1046, Test Accuracy: 0.9661\n","Finished Training Successfully\n","Accuracy train:\n","[0.5223214285714286, 0.7857142857142857, 0.9151785714285714, 0.909375, 0.9375, 0.9485294117647058, 0.9538690476190477, 0.9674479166666666, 0.9745370370370371, 0.967741935483871, 0.9742647058823529, 0.9786184210526315, 0.96875, 0.9888392857142857, 0.9965277777777778, 0.9951388888888889, 0.9955357142857143, 0.9942602040816326, 0.9963942307692307, 0.9951923076923077, 0.9949776785714286, 0.9977678571428571, 0.9941737288135594, 0.993114406779661, 0.9975198412698413, 0.9995039682539683, 0.9957386363636364, 0.9995265151515151, 0.9973214285714286, 0.9848214285714286, 0.99375, 0.9910714285714286, 0.9883928571428572, 0.9861607142857143, 0.9919642857142857, 0.9977678571428571, 0.9995535714285714, 0.9973214285714286, 0.9982142857142857, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9986607142857142, 1.0, 0.9991071428571429, 0.996875, 0.9977678571428571, 0.9986607142857142, 0.9995535714285714, 1.0, 0.9991071428571429, 0.9919642857142857, 0.9924107142857143, 0.9933035714285714, 0.9897321428571428, 0.9866071428571429, 0.9928571428571429, 0.9986607142857142, 1.0, 0.9995535714285714, 1.0, 1.0, 1.0, 1.0, 0.9986607142857142, 0.996875, 0.9982142857142857, 0.9950892857142857, 0.9928571428571429, 0.9941964285714285, 0.9955357142857143, 0.9973214285714286, 0.9986607142857142, 0.996875, 0.9915178571428571, 0.9991071428571429, 0.9982142857142857, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n","Accuracy test:\n","[0.5571428571428572, 0.7607142857142857, 0.8214285714285714, 0.8910714285714286, 0.9196428571428571, 0.9285714285714286, 0.9285714285714286, 0.9267857142857143, 0.9446428571428571, 0.9339285714285714, 0.9535714285714286, 0.9482142857142857, 0.9392857142857143, 0.9517857142857142, 0.9625, 0.9553571428571429, 0.9428571428571428, 0.9571428571428572, 0.9482142857142857, 0.95, 0.9410714285714286, 0.9535714285714286, 0.9232142857142858, 0.9392857142857143, 0.9428571428571428, 0.9535714285714286, 0.9571428571428572, 0.9571428571428572, 0.9571428571428572, 0.9303571428571429, 0.9410714285714286, 0.9517857142857142, 0.9357142857142857, 0.9142857142857143, 0.9464285714285714, 0.9535714285714286, 0.9642857142857143, 0.9571428571428572, 0.9535714285714286, 0.9607142857142857, 0.9553571428571429, 0.9589285714285715, 0.9678571428571429, 0.9607142857142857, 0.9553571428571429, 0.9589285714285715, 0.9535714285714286, 0.9446428571428571, 0.9428571428571428, 0.9535714285714286, 0.9517857142857142, 0.9535714285714286, 0.9232142857142858, 0.9232142857142858, 0.9375, 0.9196428571428571, 0.9196428571428571, 0.9446428571428571, 0.9464285714285714, 0.95, 0.9571428571428572, 0.9535714285714286, 0.9571428571428572, 0.9589285714285715, 0.9589285714285715, 0.9625, 0.9517857142857142, 0.9482142857142857, 0.9553571428571429, 0.9303571428571429, 0.9196428571428571, 0.9482142857142857, 0.95, 0.9660714285714286, 0.9571428571428572, 0.9517857142857142, 0.9428571428571428, 0.9428571428571428, 0.9535714285714286, 0.9553571428571429, 0.9589285714285715, 0.9553571428571429, 0.9607142857142857, 0.9571428571428572, 0.9553571428571429, 0.9589285714285715, 0.9607142857142857, 0.9607142857142857, 0.9625, 0.9625, 0.9589285714285715, 0.9589285714285715, 0.9625, 0.9589285714285715, 0.9607142857142857, 0.9589285714285715, 0.9625, 0.9589285714285715, 0.9642857142857143, 0.9660714285714286]\n","Loss train:\n","[1.3892102922712053, 0.6462090185710362, 0.36381528207233976, 0.30002729445695875, 0.2142133265733719, 0.16550279036164284, 0.1742092019745282, 0.10599540399077038, 0.0892216779153656, 0.10140569568161041, 0.09154874908135217, 0.07676830797113086, 0.08588518817642969, 0.04216166488116696, 0.019527197463644876, 0.02339662354853418, 0.022151064286863774, 0.02737971715039897, 0.016857481313099224, 0.01930086347811784, 0.01982211296438306, 0.01365652668131848, 0.01749607806796429, 0.025641064877795466, 0.014801533296815165, 0.006875711419279613, 0.02105575502641979, 0.005421239490246851, 0.02106521713514147, 0.05151334621145257, 0.024669071967114826, 0.02480868839899943, 0.03406354695990948, 0.03288981307663822, 0.026072975719996196, 0.013837456151044794, 0.00424228141491767, 0.009610163206941382, 0.007475655647327325, 0.0016223100587792162, 0.0016691626201333876, 0.0007371305484931717, 0.0008500558100682351, 0.0009172741814316915, 0.003407963734623211, 0.0023445868014407875, 0.0037093619434537166, 0.012102465990132519, 0.007949563117290382, 0.005797272383850733, 0.003224717557688044, 0.001506527093754682, 0.003966634405307039, 0.021419366704295888, 0.026917419246664003, 0.02134431581611612, 0.02867660978808999, 0.03897207081650517, 0.023245372278116908, 0.005019212442649795, 0.0023947653657939686, 0.0013210627943668182, 0.0012035949682675502, 0.001848950439307373, 0.0006747767919607992, 0.0011096964249840571, 0.0033384887137799524, 0.010294347285162075, 0.005213291137728707, 0.014034156454207343, 0.021574239117450947, 0.0176264064292939, 0.016588928722194397, 0.007357541243774384, 0.0035925854983880918, 0.008430950271368991, 0.024222147564432816, 0.009095344596633886, 0.008014671564420235, 0.0021472192900546362, 0.001504638510622109, 0.0004286275669333658, 0.0007806504880136344, 0.00030635308480019116, 0.0005833933581275882, 0.00036881686254283913, 0.00022799606826343473, 0.0002582062394399795, 0.0002442759978293907, 0.00048255666505221076, 0.00033381514318274897, 0.00024306433386820053, 0.0008254469380257692, 0.0002178125606535884, 0.00039800226255205676, 0.00015133548961655054, 0.00016496562241497616, 0.00025259612749713205, 0.00011336176131797921, 0.00014863406938405075]\n","Loss test:\n","[1.1493770088468278, 0.6834325637136187, 0.48621132373809817, 0.31849579172474995, 0.2382577657699585, 0.19136274456977845, 0.20149896017142704, 0.19694061172860008, 0.15221131167241506, 0.18903687936919075, 0.14947612924235207, 0.1571255071886948, 0.18078547247818538, 0.1551939274583544, 0.12128730043768883, 0.13369621166161128, 0.2019699724657195, 0.15347664462668556, 0.15128367064254625, 0.14630785698869397, 0.18485283404588698, 0.16752005549413818, 0.22818949690886906, 0.1879859162228448, 0.19344733003526926, 0.15331853750560964, 0.13336199966392348, 0.12629231988851514, 0.12260354869067669, 0.2252114459872246, 0.19787207756723676, 0.17398835846355984, 0.19190054964274167, 0.30813284324748175, 0.17851010438586984, 0.13003787366407257, 0.11566017993858882, 0.1382146774658135, 0.12310052407639367, 0.13343531191349028, 0.1351906750883375, 0.12599767478449003, 0.13464919089206628, 0.12033187731036118, 0.13259099242942674, 0.13133785990732058, 0.15350746501769338, 0.15333615413733892, 0.1924527902688299, 0.14926791191101074, 0.15760940898741996, 0.14677356715713227, 0.2876373680574553, 0.34874214679002763, 0.23139039309961454, 0.28036045751401356, 0.2863684294479234, 0.22094390434878214, 0.17240887380072048, 0.18609413749405315, 0.15103125970012377, 0.1438608848622867, 0.15646167593076826, 0.1508515710809401, 0.13874335748010447, 0.14177395021542907, 0.19221715120864766, 0.20910028340294956, 0.1832298700564674, 0.33915323913097384, 0.3127599141427449, 0.19380994383245706, 0.1676687647455505, 0.09968658289206879, 0.12512346238696148, 0.15545382776430675, 0.18722580032689232, 0.1539614923298359, 0.1529966169169971, 0.12259294659431491, 0.13165315030408756, 0.1228556629137269, 0.11390007699706725, 0.10936177130788564, 0.10589576368885381, 0.10979664966996228, 0.10775675866752862, 0.10848238005169801, 0.10460415474538293, 0.10431888162025384, 0.11933355697297625, 0.11380138179021222, 0.11362248063087463, 0.11464915898229394, 0.10936831294425896, 0.10214545921023403, 0.10629126488763307, 0.10588288916540997, 0.10949783420988492, 0.10456319998151489]\n","Time epoch:\n","[29.14161992073059, 51.343236684799194, 71.56899857521057, 91.20915818214417, 111.75172162055969, 133.7437174320221, 155.0716516971588, 177.116934299469, 199.46121382713318, 221.85172414779663, 245.11028265953064, 268.7471480369568, 292.2088143825531, 316.0062744617462, 340.26769733428955, 364.47625756263733, 388.8634443283081, 413.0909481048584, 438.2377293109894, 463.14677357673645, 488.6177930831909, 513.6222138404846, 538.8714935779572, 564.8781659603119, 590.9900159835815, 617.2531945705414, 643.1724059581757, 669.6878020763397, 692.8624093532562, 716.0798790454865, 739.7475986480713, 762.8635733127594, 786.4691784381866, 810.0260229110718, 833.4274079799652, 857.0395150184631, 880.262538433075, 903.5968849658966, 926.9106171131134, 950.3050518035889, 973.7695441246033, 997.3084523677826, 1020.6925764083862, 1044.5770213603973, 1067.8473868370056, 1091.334888935089, 1114.534492969513, 1137.9558837413788, 1161.6044330596924, 1184.7320866584778, 1208.2547888755798, 1231.4890022277832, 1254.6033356189728, 1278.1147303581238, 1301.171588897705, 1324.2375938892365, 1347.7374002933502, 1370.768256187439, 1394.519568681717, 1417.4789304733276, 1440.6498000621796, 1464.1083748340607, 1487.3412516117096, 1510.6809484958649, 1534.0798225402832, 1557.9435572624207, 1581.2733509540558, 1604.289016008377, 1627.924393415451, 1651.326877117157, 1674.6061372756958, 1697.8570997714996, 1721.045037984848, 1744.334604024887, 1767.729784488678, 1790.7971303462982, 1814.1073331832886, 1837.4033498764038, 1860.8129534721375, 1883.8987839221954, 1907.2815823554993, 1930.589571237564, 1953.6836020946503, 1977.0412547588348, 2000.5530083179474, 2023.6245052814484, 2047.1507251262665, 2070.6167833805084, 2093.885445833206, 2117.688535451889, 2140.8804700374603, 2164.6677935123444, 2187.853620290756, 2210.9031839370728, 2234.3213889598846, 2257.4369733333588, 2280.638865709305, 2304.0329201221466, 2326.984890937805, 2350.8041021823883]\n","Learning rate:\n","[0.0001, 0.0001, 1e-05, 1e-05, 1e-06, 1e-06, 1e-06, 1e-07, 1e-07, 1e-07, 1e-07, 1e-07, 1e-07, 1e-07, 1e-07, 1e-07, 1e-07, 1e-07, 1e-07, 1e-07, 1e-07, 1e-07, 1e-07, 1e-07, 1e-07, 1e-07, 1e-07, 1e-07, 1e-07, 1e-07, 1e-07, 1e-07, 1e-07, 1e-07, 1e-07, 1e-07, 1e-07, 1e-07, 1e-07, 1e-07, 1e-07, 1e-07, 1e-07, 1e-07, 1e-07, 1e-07, 1e-07, 1e-07, 1e-07, 1e-07, 1e-07, 1e-07, 1e-07, 1e-07, 1e-07, 1e-07, 1e-07, 1e-07, 1e-07, 1e-07, 1e-07, 1e-07, 1e-07, 1e-07, 1e-07, 1e-07, 1e-07, 1e-07, 1e-07, 1e-07, 1e-07, 1e-07, 1e-07, 1e-07, 1e-07, 1e-07, 1e-07, 1e-07, 1e-07, 1e-07, 1e-07, 1e-07, 1e-07, 1e-07, 1e-07, 1e-07, 1e-07, 1e-07, 1e-07, 1e-07, 1e-07, 1e-07, 1e-07, 1e-07, 1e-07, 1e-07, 1e-07, 1e-07, 1e-07, 1e-07]\n","Lambdas:\n","[0.1, 0.1, 0.15000000000000002, 0.2, 0.25, 0.3, 0.35, 0.39999999999999997, 0.44999999999999996, 0.49999999999999994, 0.5499999999999999, 0.6, 0.6, 0.65, 0.65, 0.7000000000000001, 0.7000000000000001, 0.7500000000000001, 0.7500000000000001, 0.8000000000000002, 0.8000000000000002, 0.8500000000000002, 0.8500000000000002, 0.9000000000000002, 0.9000000000000002, 0.9500000000000003, 0.9500000000000003, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"]}],"source":["from torchvision.models import resnet18\n","# from ..Resnet18.task.model import ResNet18\n","import torch\n","from torch.utils.data import DataLoader, TensorDataset\n","import torch.nn as nn\n","import torch.optim as optim\n","import time\n","import random\n","\n","acc_train = []\n","acc_test = []\n","loss_train =[]\n","loss_test = []\n","time_epoch = []\n","cur_lambda = []\n","cur_learning_rate = []\n","\n","random.seed(42)\n","\n","time0 = time.time()\n","\n","data_dir = '/kaggle/input/rssnc7/RSSCN7'\n","batch_size = 32\n","learning_rate = 0.0001\n","num_epochs = 100\n","lambda_beginning = 0.1\n","lambda_end = 1\n","\n","rsscn7_data_loader = RSSCN7_DataLoader(data_dir, batch_size=batch_size, shuffle=True)\n","train_loader = rsscn7_data_loader.get_train_dataloader()\n","test_loader = rsscn7_data_loader.get_test_dataloader()\n","\n","model = resnet18(weights='ResNet18_Weights.DEFAULT')\n","num_filters = model.fc.in_features\n","model.fc = nn.Linear(num_filters, 7)\n","\n","######### In case of model pretrained on DTD: uploading the weights #################################################\n","\n","# pretrained_model_path = \"/kaggle/input/resnet18-pretrained-on-dtd/pytorch/version1/1/resnet18_trained_on_DTD_from_80_to_90.pth\"\n","# pretrained_resnet18 = ResNet18()\n","# pretrained_resnet18.load_state_dict(torch.load(pretrained_model_path, map_location=torch.device(device)))\n","\n","# model = pretrained_resnet18.to(device)\n","\n","# model.fc = nn.Linear(47, 7)\n","\n","criterion = nn.CrossEntropyLoss()\n","opitmizer = optim.Adam(model.parameters(), lr=learning_rate)\n","my_device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\")\n","\n","step = 0.05\n","\n","def train_model_self_paced(model, train_loader, test_loader, criterion, optimizer, num_epochs, learning_rate):\n","    device = my_device\n","    model.to(device)\n","    counter = 0\n","\n","    lambda_current = lambda_beginning\n","\n","    for epoch in range(num_epochs):\n","        model.train()\n","        total_loss = 0.0\n","        correct = 0\n","        total = 0\n","        train_samples = []\n","\n","        if lambda_current < 1:\n","            with torch.no_grad():\n","                for inputs, labels in train_loader:\n","                    inputs, labels = inputs.to(device), labels.to(device)\n","                    outputs = model(inputs)\n","                    loss = criterion(outputs, labels)\n","                    train_samples.append((inputs, labels, loss.item()))\n","\n","            train_samples.sort(key=lambda x: x[2])  # sort by loss (the first are the easiest)\n","\n","            num_samples_current = int(lambda_current * len(train_samples))\n","\n","            easy_enough_samples = train_samples[:num_samples_current]\n","            easy_enough_inputs = torch.cat([x[0] for x in easy_enough_samples])\n","            easy_enough_labels = torch.cat([x[1] for x in easy_enough_samples])\n","            easy_enough_dataset = TensorDataset(easy_enough_inputs, easy_enough_labels)\n","            easy_enough_loader = DataLoader(easy_enough_dataset, batch_size=batch_size, shuffle=True)\n","        else:\n","            easy_enough_loader = train_loader\n","\n","        for inputs, labels in easy_enough_loader:\n","            inputs, labels = inputs.to(device), labels.to(device)\n","\n","            optimizer.zero_grad()\n","            outputs = model(inputs)\n","            loss = criterion(outputs, labels)\n","            loss.backward()\n","            optimizer.step()\n","\n","            total_loss += loss.item() * inputs.size(0)\n","\n","            _, predicted = torch.max(outputs, 1)\n","            total += labels.size(0)\n","            correct += (predicted == labels).sum().item()\n","\n","        train_loss = total_loss / len(easy_enough_loader.dataset)\n","        train_accuracy = correct / total\n","        num_images = len(easy_enough_loader.dataset)\n","\n","        if train_accuracy >= 0.88:\n","            learning_rate = 0.00001\n","        \n","        if train_accuracy >= 0.93:\n","            learning_rate = 0.000001\n","            \n","        if train_accuracy >= 0.96:\n","            learning_rate = 0.0000001\n","\n","        cur_time_ = time.time() - time0\n","\n","        print(\"learing_rate = \", learning_rate)\n","\n","        print(\n","            f'Epoch [{epoch + 1}/{num_epochs}], Loss: {train_loss:.4f}, Accuracy: {train_accuracy:.4f}, Images: {num_images}, Lambda: {lambda_current:.2f}, Time: {cur_time_:.2f} seconds')\n","\n","        if train_accuracy > 0.85:\n","            if lambda_current < 0.6:\n","                lambda_current += step\n","                if lambda_current>1:\n","                    lambda_current = 1\n","            else:\n","                counter = counter + 1\n","                if counter % 2 == 0:\n","                    lambda_current += step\n","                    counter = 0\n","                    if lambda_current>1:\n","                        lambda_current = 1\n","\n","        acc_train.append(train_accuracy)\n","        loss_train.append(train_loss)\n","        time_epoch.append(cur_time_)\n","        cur_lambda.append(lambda_current)\n","        cur_learning_rate.append(learning_rate)\n","\n","        evaluate_model(model, test_loader, criterion)\n","\n","    print('Finished Training Successfully')\n","\n","\n","def evaluate_model(model, test_loader, criterion):\n","    model.eval()\n","    device = next(model.parameters()).device\n","    total_loss = 0.0\n","    correct = 0\n","    total = 0\n","\n","    with torch.no_grad():\n","        for inputs, labels in test_loader:\n","            inputs, labels = inputs.to(device), labels.to(device)\n","            outputs = model(inputs)\n","            loss = criterion(outputs, labels)\n","            total_loss += loss.item() * inputs.size(0)\n","\n","            _, predicted = torch.max(outputs, 1)\n","            total += labels.size(0)\n","            correct += (predicted == labels).sum().item()\n","\n","    test_loss = total_loss / len(test_loader.dataset)\n","    test_accuracy = correct / total\n","\n","    print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}')\n","\n","    acc_test.append(test_accuracy)\n","    loss_test.append(test_loss)\n","\n","\n","train_model_self_paced(model, train_loader, test_loader, criterion, opitmizer, num_epochs, learning_rate)\n","print(\"Accuracy train:\")\n","print(acc_train)\n","print(\"Accuracy test:\")\n","print(acc_test)\n","print(\"Loss train:\")\n","print(loss_train)\n","print(\"Loss test:\")\n","print(loss_test)\n","print(\"Time epoch:\")\n","print(time_epoch)\n","print(\"Learning rate:\")\n","print(cur_learning_rate)\n","print(\"Lambdas:\")\n","print(cur_lambda)\n","\n","torch.save(model, 'resnet18_imagenet_self_paced.pth')"]},{"cell_type":"markdown","metadata":{},"source":["DTD, self paced"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-05-28T16:34:48.042283Z","iopub.status.busy":"2024-05-28T16:34:48.041793Z","iopub.status.idle":"2024-05-28T17:14:03.320768Z","shell.execute_reply":"2024-05-28T17:14:03.319668Z","shell.execute_reply.started":"2024-05-28T16:34:48.042252Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n","  warnings.warn(msg)\n"]},{"name":"stdout","output_type":"stream","text":["learing_rate =  0.0001\n","Epoch [1/100], Loss: 6.1519, Accuracy: 0.0625, Images: 224, Lambda: 0.10, Time: 27.19 seconds\n","Test Loss: 6.8008, Test Accuracy: 0.0768\n","learing_rate =  0.0001\n","Epoch [2/100], Loss: 4.6739, Accuracy: 0.1562, Images: 224, Lambda: 0.10, Time: 48.98 seconds\n","Test Loss: 5.7303, Test Accuracy: 0.1375\n","learing_rate =  0.0001\n","Epoch [3/100], Loss: 4.1179, Accuracy: 0.2455, Images: 224, Lambda: 0.10, Time: 69.30 seconds\n","Test Loss: 4.8093, Test Accuracy: 0.2286\n","learing_rate =  0.0001\n","Epoch [4/100], Loss: 3.2014, Accuracy: 0.3527, Images: 224, Lambda: 0.10, Time: 89.01 seconds\n","Test Loss: 4.0985, Test Accuracy: 0.3232\n","learing_rate =  0.0001\n","Epoch [5/100], Loss: 2.6233, Accuracy: 0.4152, Images: 224, Lambda: 0.10, Time: 108.89 seconds\n","Test Loss: 3.4448, Test Accuracy: 0.3875\n","learing_rate =  0.0001\n","Epoch [6/100], Loss: 2.2069, Accuracy: 0.5089, Images: 224, Lambda: 0.10, Time: 129.96 seconds\n","Test Loss: 2.9643, Test Accuracy: 0.4554\n","learing_rate =  0.0001\n","Epoch [7/100], Loss: 1.6504, Accuracy: 0.5759, Images: 224, Lambda: 0.10, Time: 150.18 seconds\n","Test Loss: 2.5608, Test Accuracy: 0.4964\n","learing_rate =  0.0001\n","Epoch [8/100], Loss: 1.4050, Accuracy: 0.6696, Images: 224, Lambda: 0.10, Time: 170.85 seconds\n","Test Loss: 2.2621, Test Accuracy: 0.5411\n","learing_rate =  0.0001\n","Epoch [9/100], Loss: 1.3541, Accuracy: 0.6652, Images: 224, Lambda: 0.10, Time: 191.36 seconds\n","Test Loss: 2.0551, Test Accuracy: 0.5304\n","learing_rate =  0.0001\n","Epoch [10/100], Loss: 1.1196, Accuracy: 0.6562, Images: 224, Lambda: 0.10, Time: 211.25 seconds\n","Test Loss: 1.8577, Test Accuracy: 0.5732\n","learing_rate =  0.0001\n","Epoch [11/100], Loss: 0.8047, Accuracy: 0.7812, Images: 224, Lambda: 0.10, Time: 231.31 seconds\n","Test Loss: 1.7265, Test Accuracy: 0.6036\n","learing_rate =  0.0001\n","Epoch [12/100], Loss: 0.9816, Accuracy: 0.7188, Images: 224, Lambda: 0.10, Time: 251.68 seconds\n","Test Loss: 1.5419, Test Accuracy: 0.6179\n","learing_rate =  0.0001\n","Epoch [13/100], Loss: 0.8911, Accuracy: 0.7411, Images: 224, Lambda: 0.10, Time: 271.53 seconds\n","Test Loss: 1.4295, Test Accuracy: 0.6518\n","learing_rate =  0.0001\n","Epoch [14/100], Loss: 0.7620, Accuracy: 0.7589, Images: 224, Lambda: 0.10, Time: 291.69 seconds\n","Test Loss: 1.3421, Test Accuracy: 0.6554\n","learing_rate =  0.0001\n","Epoch [15/100], Loss: 0.8484, Accuracy: 0.7634, Images: 224, Lambda: 0.10, Time: 312.19 seconds\n","Test Loss: 1.2974, Test Accuracy: 0.6518\n","learing_rate =  0.0001\n","Epoch [16/100], Loss: 0.6451, Accuracy: 0.7991, Images: 224, Lambda: 0.10, Time: 332.19 seconds\n","Test Loss: 1.2003, Test Accuracy: 0.6875\n","learing_rate =  0.0001\n","Epoch [17/100], Loss: 0.6500, Accuracy: 0.8125, Images: 224, Lambda: 0.10, Time: 352.24 seconds\n","Test Loss: 1.1209, Test Accuracy: 0.7018\n","learing_rate =  0.0001\n","Epoch [18/100], Loss: 0.6313, Accuracy: 0.7969, Images: 320, Lambda: 0.15, Time: 372.95 seconds\n","Test Loss: 1.0687, Test Accuracy: 0.7107\n","learing_rate =  0.0001\n","Epoch [19/100], Loss: 0.5900, Accuracy: 0.8031, Images: 320, Lambda: 0.15, Time: 393.48 seconds\n","Test Loss: 1.0104, Test Accuracy: 0.7268\n","learing_rate =  0.0001\n","Epoch [20/100], Loss: 0.5591, Accuracy: 0.8259, Images: 448, Lambda: 0.20, Time: 414.81 seconds\n","Test Loss: 0.9703, Test Accuracy: 0.7268\n","learing_rate =  0.0001\n","Epoch [21/100], Loss: 0.5575, Accuracy: 0.8309, Images: 544, Lambda: 0.25, Time: 436.49 seconds\n","Test Loss: 0.9077, Test Accuracy: 0.7268\n","learing_rate =  0.0001\n","Epoch [22/100], Loss: 0.5595, Accuracy: 0.8259, Images: 672, Lambda: 0.30, Time: 458.22 seconds\n","Test Loss: 0.8170, Test Accuracy: 0.7536\n","learing_rate =  0.0001\n","Epoch [23/100], Loss: 0.4052, Accuracy: 0.8607, Images: 768, Lambda: 0.35, Time: 480.61 seconds\n","Test Loss: 0.7689, Test Accuracy: 0.7768\n","learing_rate =  0.0001\n","Epoch [24/100], Loss: 0.4802, Accuracy: 0.8495, Images: 864, Lambda: 0.40, Time: 503.17 seconds\n","Test Loss: 0.6984, Test Accuracy: 0.7893\n","learing_rate =  0.0001\n","Epoch [25/100], Loss: 0.4163, Accuracy: 0.8528, Images: 992, Lambda: 0.45, Time: 525.99 seconds\n","Test Loss: 0.6637, Test Accuracy: 0.7946\n","learing_rate =  0.0001\n","Epoch [26/100], Loss: 0.3913, Accuracy: 0.8722, Images: 1088, Lambda: 0.50, Time: 549.90 seconds\n","Test Loss: 0.6307, Test Accuracy: 0.8089\n","learing_rate =  1e-05\n","Epoch [27/100], Loss: 0.3399, Accuracy: 0.8865, Images: 1216, Lambda: 0.55, Time: 573.77 seconds\n","Test Loss: 0.5627, Test Accuracy: 0.8321\n","learing_rate =  1e-05\n","Epoch [28/100], Loss: 0.3361, Accuracy: 0.8839, Images: 1344, Lambda: 0.60, Time: 597.39 seconds\n","Test Loss: 0.5516, Test Accuracy: 0.8357\n","learing_rate =  1e-05\n","Epoch [29/100], Loss: 0.3372, Accuracy: 0.8847, Images: 1344, Lambda: 0.60, Time: 621.44 seconds\n","Test Loss: 0.5088, Test Accuracy: 0.8393\n","learing_rate =  1e-05\n","Epoch [30/100], Loss: 0.3022, Accuracy: 0.8896, Images: 1440, Lambda: 0.65, Time: 645.54 seconds\n","Test Loss: 0.4727, Test Accuracy: 0.8446\n","learing_rate =  1e-05\n","Epoch [31/100], Loss: 0.2305, Accuracy: 0.9125, Images: 1440, Lambda: 0.65, Time: 670.25 seconds\n","Test Loss: 0.4750, Test Accuracy: 0.8464\n","learing_rate =  1e-05\n","Epoch [32/100], Loss: 0.2351, Accuracy: 0.9171, Images: 1568, Lambda: 0.70, Time: 695.14 seconds\n","Test Loss: 0.4393, Test Accuracy: 0.8554\n","learing_rate =  1e-05\n","Epoch [33/100], Loss: 0.2254, Accuracy: 0.9228, Images: 1568, Lambda: 0.70, Time: 719.90 seconds\n","Test Loss: 0.4055, Test Accuracy: 0.8589\n","learing_rate =  1e-06\n","Epoch [34/100], Loss: 0.1914, Accuracy: 0.9375, Images: 1664, Lambda: 0.75, Time: 745.24 seconds\n","Test Loss: 0.4280, Test Accuracy: 0.8589\n","learing_rate =  1e-06\n","Epoch [35/100], Loss: 0.1815, Accuracy: 0.9375, Images: 1664, Lambda: 0.75, Time: 770.29 seconds\n","Test Loss: 0.3886, Test Accuracy: 0.8679\n","learing_rate =  1e-06\n","Epoch [36/100], Loss: 0.1707, Accuracy: 0.9442, Images: 1792, Lambda: 0.80, Time: 796.13 seconds\n","Test Loss: 0.4186, Test Accuracy: 0.8661\n","learing_rate =  1e-06\n","Epoch [37/100], Loss: 0.1699, Accuracy: 0.9442, Images: 1792, Lambda: 0.80, Time: 821.41 seconds\n","Test Loss: 0.3987, Test Accuracy: 0.8786\n","learing_rate =  1e-06\n","Epoch [38/100], Loss: 0.1314, Accuracy: 0.9576, Images: 1888, Lambda: 0.85, Time: 847.46 seconds\n","Test Loss: 0.3658, Test Accuracy: 0.8821\n","learing_rate =  1e-06\n","Epoch [39/100], Loss: 0.1373, Accuracy: 0.9507, Images: 1888, Lambda: 0.85, Time: 873.43 seconds\n","Test Loss: 0.3726, Test Accuracy: 0.8732\n","learing_rate =  1e-07\n","Epoch [40/100], Loss: 0.1048, Accuracy: 0.9683, Images: 2016, Lambda: 0.90, Time: 899.69 seconds\n","Test Loss: 0.3845, Test Accuracy: 0.8821\n","learing_rate =  1e-07\n","Epoch [41/100], Loss: 0.1009, Accuracy: 0.9683, Images: 2016, Lambda: 0.90, Time: 926.33 seconds\n","Test Loss: 0.3731, Test Accuracy: 0.8857\n","learing_rate =  1e-07\n","Epoch [42/100], Loss: 0.1047, Accuracy: 0.9664, Images: 2112, Lambda: 0.95, Time: 952.64 seconds\n","Test Loss: 0.3658, Test Accuracy: 0.8821\n","learing_rate =  1e-07\n","Epoch [43/100], Loss: 0.0889, Accuracy: 0.9692, Images: 2112, Lambda: 0.95, Time: 979.03 seconds\n","Test Loss: 0.3586, Test Accuracy: 0.8804\n","learing_rate =  1e-07\n","Epoch [44/100], Loss: 0.0913, Accuracy: 0.9754, Images: 2240, Lambda: 1.00, Time: 1002.14 seconds\n","Test Loss: 0.3686, Test Accuracy: 0.8786\n","learing_rate =  1e-07\n","Epoch [45/100], Loss: 0.0811, Accuracy: 0.9790, Images: 2240, Lambda: 1.00, Time: 1025.81 seconds\n","Test Loss: 0.3332, Test Accuracy: 0.8911\n","learing_rate =  1e-07\n","Epoch [46/100], Loss: 0.0742, Accuracy: 0.9781, Images: 2240, Lambda: 1.00, Time: 1049.18 seconds\n","Test Loss: 0.3474, Test Accuracy: 0.8964\n","learing_rate =  1e-07\n","Epoch [47/100], Loss: 0.0623, Accuracy: 0.9853, Images: 2240, Lambda: 1.00, Time: 1072.81 seconds\n","Test Loss: 0.3400, Test Accuracy: 0.8964\n","learing_rate =  1e-07\n","Epoch [48/100], Loss: 0.0638, Accuracy: 0.9830, Images: 2240, Lambda: 1.00, Time: 1096.49 seconds\n","Test Loss: 0.3526, Test Accuracy: 0.8821\n","learing_rate =  1e-07\n","Epoch [49/100], Loss: 0.0515, Accuracy: 0.9862, Images: 2240, Lambda: 1.00, Time: 1120.02 seconds\n","Test Loss: 0.3516, Test Accuracy: 0.8839\n","learing_rate =  1e-07\n","Epoch [50/100], Loss: 0.0432, Accuracy: 0.9888, Images: 2240, Lambda: 1.00, Time: 1143.69 seconds\n","Test Loss: 0.3326, Test Accuracy: 0.8946\n","learing_rate =  1e-07\n","Epoch [51/100], Loss: 0.0454, Accuracy: 0.9897, Images: 2240, Lambda: 1.00, Time: 1167.49 seconds\n","Test Loss: 0.3354, Test Accuracy: 0.8929\n","learing_rate =  1e-07\n","Epoch [52/100], Loss: 0.0407, Accuracy: 0.9902, Images: 2240, Lambda: 1.00, Time: 1190.74 seconds\n","Test Loss: 0.3388, Test Accuracy: 0.8946\n","learing_rate =  1e-07\n","Epoch [53/100], Loss: 0.0370, Accuracy: 0.9924, Images: 2240, Lambda: 1.00, Time: 1214.41 seconds\n","Test Loss: 0.3446, Test Accuracy: 0.8911\n","learing_rate =  1e-07\n","Epoch [54/100], Loss: 0.0337, Accuracy: 0.9929, Images: 2240, Lambda: 1.00, Time: 1237.97 seconds\n","Test Loss: 0.3425, Test Accuracy: 0.8964\n","learing_rate =  1e-07\n","Epoch [55/100], Loss: 0.0343, Accuracy: 0.9911, Images: 2240, Lambda: 1.00, Time: 1261.54 seconds\n","Test Loss: 0.3293, Test Accuracy: 0.8911\n","learing_rate =  1e-07\n","Epoch [56/100], Loss: 0.0274, Accuracy: 0.9942, Images: 2240, Lambda: 1.00, Time: 1284.77 seconds\n","Test Loss: 0.3586, Test Accuracy: 0.9000\n","learing_rate =  1e-07\n","Epoch [57/100], Loss: 0.0325, Accuracy: 0.9933, Images: 2240, Lambda: 1.00, Time: 1308.25 seconds\n","Test Loss: 0.3614, Test Accuracy: 0.9018\n","learing_rate =  1e-07\n","Epoch [58/100], Loss: 0.0237, Accuracy: 0.9951, Images: 2240, Lambda: 1.00, Time: 1331.97 seconds\n","Test Loss: 0.3305, Test Accuracy: 0.8964\n","learing_rate =  1e-07\n","Epoch [59/100], Loss: 0.0277, Accuracy: 0.9938, Images: 2240, Lambda: 1.00, Time: 1355.45 seconds\n","Test Loss: 0.3187, Test Accuracy: 0.9000\n","learing_rate =  1e-07\n","Epoch [60/100], Loss: 0.0243, Accuracy: 0.9960, Images: 2240, Lambda: 1.00, Time: 1378.80 seconds\n","Test Loss: 0.3188, Test Accuracy: 0.9071\n","learing_rate =  1e-07\n","Epoch [61/100], Loss: 0.0170, Accuracy: 0.9973, Images: 2240, Lambda: 1.00, Time: 1402.60 seconds\n","Test Loss: 0.3414, Test Accuracy: 0.8964\n","learing_rate =  1e-07\n","Epoch [62/100], Loss: 0.0177, Accuracy: 0.9960, Images: 2240, Lambda: 1.00, Time: 1425.82 seconds\n","Test Loss: 0.3447, Test Accuracy: 0.9054\n","learing_rate =  1e-07\n","Epoch [63/100], Loss: 0.0192, Accuracy: 0.9946, Images: 2240, Lambda: 1.00, Time: 1449.84 seconds\n","Test Loss: 0.3528, Test Accuracy: 0.9018\n","learing_rate =  1e-07\n","Epoch [64/100], Loss: 0.0204, Accuracy: 0.9942, Images: 2240, Lambda: 1.00, Time: 1472.99 seconds\n","Test Loss: 0.3708, Test Accuracy: 0.8875\n","learing_rate =  1e-07\n","Epoch [65/100], Loss: 0.0140, Accuracy: 0.9978, Images: 2240, Lambda: 1.00, Time: 1497.12 seconds\n","Test Loss: 0.4469, Test Accuracy: 0.8929\n","learing_rate =  1e-07\n","Epoch [66/100], Loss: 0.0193, Accuracy: 0.9955, Images: 2240, Lambda: 1.00, Time: 1521.18 seconds\n","Test Loss: 0.3312, Test Accuracy: 0.9179\n","learing_rate =  1e-07\n","Epoch [67/100], Loss: 0.0191, Accuracy: 0.9951, Images: 2240, Lambda: 1.00, Time: 1544.77 seconds\n","Test Loss: 0.3474, Test Accuracy: 0.8964\n","learing_rate =  1e-07\n","Epoch [68/100], Loss: 0.0185, Accuracy: 0.9955, Images: 2240, Lambda: 1.00, Time: 1568.55 seconds\n","Test Loss: 0.3374, Test Accuracy: 0.8964\n","learing_rate =  1e-07\n","Epoch [69/100], Loss: 0.0123, Accuracy: 0.9982, Images: 2240, Lambda: 1.00, Time: 1592.10 seconds\n","Test Loss: 0.3815, Test Accuracy: 0.8911\n","learing_rate =  1e-07\n","Epoch [70/100], Loss: 0.0180, Accuracy: 0.9955, Images: 2240, Lambda: 1.00, Time: 1615.71 seconds\n","Test Loss: 0.3534, Test Accuracy: 0.9107\n","learing_rate =  1e-07\n","Epoch [71/100], Loss: 0.0124, Accuracy: 0.9978, Images: 2240, Lambda: 1.00, Time: 1639.61 seconds\n","Test Loss: 0.3759, Test Accuracy: 0.9000\n","learing_rate =  1e-07\n","Epoch [72/100], Loss: 0.0116, Accuracy: 0.9978, Images: 2240, Lambda: 1.00, Time: 1663.11 seconds\n","Test Loss: 0.3542, Test Accuracy: 0.8911\n","learing_rate =  1e-07\n","Epoch [73/100], Loss: 0.0162, Accuracy: 0.9951, Images: 2240, Lambda: 1.00, Time: 1687.41 seconds\n","Test Loss: 0.3427, Test Accuracy: 0.9071\n","learing_rate =  1e-07\n","Epoch [74/100], Loss: 0.0128, Accuracy: 0.9973, Images: 2240, Lambda: 1.00, Time: 1710.75 seconds\n","Test Loss: 0.3663, Test Accuracy: 0.9036\n","learing_rate =  1e-07\n","Epoch [75/100], Loss: 0.0173, Accuracy: 0.9955, Images: 2240, Lambda: 1.00, Time: 1734.43 seconds\n","Test Loss: 0.3727, Test Accuracy: 0.9018\n","learing_rate =  1e-07\n","Epoch [76/100], Loss: 0.0129, Accuracy: 0.9964, Images: 2240, Lambda: 1.00, Time: 1758.77 seconds\n","Test Loss: 0.3654, Test Accuracy: 0.9018\n","learing_rate =  1e-07\n","Epoch [77/100], Loss: 0.0129, Accuracy: 0.9973, Images: 2240, Lambda: 1.00, Time: 1782.44 seconds\n","Test Loss: 0.3773, Test Accuracy: 0.8982\n","learing_rate =  1e-07\n","Epoch [78/100], Loss: 0.0102, Accuracy: 0.9973, Images: 2240, Lambda: 1.00, Time: 1806.42 seconds\n","Test Loss: 0.3663, Test Accuracy: 0.9018\n","learing_rate =  1e-07\n","Epoch [79/100], Loss: 0.0095, Accuracy: 0.9987, Images: 2240, Lambda: 1.00, Time: 1830.47 seconds\n","Test Loss: 0.3398, Test Accuracy: 0.9107\n","learing_rate =  1e-07\n","Epoch [80/100], Loss: 0.0157, Accuracy: 0.9964, Images: 2240, Lambda: 1.00, Time: 1856.36 seconds\n","Test Loss: 0.3695, Test Accuracy: 0.9036\n","learing_rate =  1e-07\n","Epoch [81/100], Loss: 0.0104, Accuracy: 0.9978, Images: 2240, Lambda: 1.00, Time: 1879.98 seconds\n","Test Loss: 0.3755, Test Accuracy: 0.8893\n","learing_rate =  1e-07\n","Epoch [82/100], Loss: 0.0089, Accuracy: 0.9982, Images: 2240, Lambda: 1.00, Time: 1903.42 seconds\n","Test Loss: 0.3620, Test Accuracy: 0.9054\n","learing_rate =  1e-07\n","Epoch [83/100], Loss: 0.0051, Accuracy: 1.0000, Images: 2240, Lambda: 1.00, Time: 1927.45 seconds\n","Test Loss: 0.3302, Test Accuracy: 0.9143\n","learing_rate =  1e-07\n","Epoch [84/100], Loss: 0.0044, Accuracy: 1.0000, Images: 2240, Lambda: 1.00, Time: 1950.99 seconds\n","Test Loss: 0.3482, Test Accuracy: 0.9143\n","learing_rate =  1e-07\n","Epoch [85/100], Loss: 0.0067, Accuracy: 0.9987, Images: 2240, Lambda: 1.00, Time: 1974.24 seconds\n","Test Loss: 0.3368, Test Accuracy: 0.9179\n","learing_rate =  1e-07\n","Epoch [86/100], Loss: 0.0064, Accuracy: 0.9991, Images: 2240, Lambda: 1.00, Time: 1998.15 seconds\n","Test Loss: 0.3525, Test Accuracy: 0.9089\n","learing_rate =  1e-07\n","Epoch [87/100], Loss: 0.0059, Accuracy: 0.9987, Images: 2240, Lambda: 1.00, Time: 2021.65 seconds\n","Test Loss: 0.3231, Test Accuracy: 0.9232\n","learing_rate =  1e-07\n","Epoch [88/100], Loss: 0.0119, Accuracy: 0.9964, Images: 2240, Lambda: 1.00, Time: 2045.62 seconds\n","Test Loss: 0.3626, Test Accuracy: 0.9125\n","learing_rate =  1e-07\n","Epoch [89/100], Loss: 0.0092, Accuracy: 0.9982, Images: 2240, Lambda: 1.00, Time: 2069.26 seconds\n","Test Loss: 0.3690, Test Accuracy: 0.9143\n","learing_rate =  1e-07\n","Epoch [90/100], Loss: 0.0068, Accuracy: 0.9991, Images: 2240, Lambda: 1.00, Time: 2092.81 seconds\n","Test Loss: 0.3409, Test Accuracy: 0.9036\n","learing_rate =  1e-07\n","Epoch [91/100], Loss: 0.0068, Accuracy: 0.9991, Images: 2240, Lambda: 1.00, Time: 2118.05 seconds\n","Test Loss: 0.3313, Test Accuracy: 0.9054\n","learing_rate =  1e-07\n","Epoch [92/100], Loss: 0.0040, Accuracy: 1.0000, Images: 2240, Lambda: 1.00, Time: 2146.93 seconds\n","Test Loss: 0.3454, Test Accuracy: 0.9179\n","learing_rate =  1e-07\n","Epoch [93/100], Loss: 0.0041, Accuracy: 1.0000, Images: 2240, Lambda: 1.00, Time: 2173.00 seconds\n","Test Loss: 0.3440, Test Accuracy: 0.9179\n","learing_rate =  1e-07\n","Epoch [94/100], Loss: 0.0057, Accuracy: 0.9987, Images: 2240, Lambda: 1.00, Time: 2200.93 seconds\n","Test Loss: 0.3260, Test Accuracy: 0.9107\n","learing_rate =  1e-07\n","Epoch [95/100], Loss: 0.0039, Accuracy: 1.0000, Images: 2240, Lambda: 1.00, Time: 2227.39 seconds\n","Test Loss: 0.3398, Test Accuracy: 0.9161\n","learing_rate =  1e-07\n","Epoch [96/100], Loss: 0.0023, Accuracy: 1.0000, Images: 2240, Lambda: 1.00, Time: 2252.16 seconds\n","Test Loss: 0.3198, Test Accuracy: 0.9143\n","learing_rate =  1e-07\n","Epoch [97/100], Loss: 0.0073, Accuracy: 0.9987, Images: 2240, Lambda: 1.00, Time: 2278.16 seconds\n","Test Loss: 0.3372, Test Accuracy: 0.9089\n","learing_rate =  1e-07\n","Epoch [98/100], Loss: 0.0071, Accuracy: 0.9987, Images: 2240, Lambda: 1.00, Time: 2304.37 seconds\n","Test Loss: 0.3248, Test Accuracy: 0.9161\n","learing_rate =  1e-07\n","Epoch [99/100], Loss: 0.0064, Accuracy: 0.9987, Images: 2240, Lambda: 1.00, Time: 2328.06 seconds\n","Test Loss: 0.3638, Test Accuracy: 0.9071\n","learing_rate =  1e-07\n","Epoch [100/100], Loss: 0.0145, Accuracy: 0.9951, Images: 2240, Lambda: 1.00, Time: 2351.52 seconds\n","Test Loss: 0.4475, Test Accuracy: 0.8929\n","Finished Training Successfully\n","Accuracy train:\n","[0.0625, 0.15625, 0.24553571428571427, 0.35267857142857145, 0.41517857142857145, 0.5089285714285714, 0.5758928571428571, 0.6696428571428571, 0.6651785714285714, 0.65625, 0.78125, 0.71875, 0.7410714285714286, 0.7589285714285714, 0.7633928571428571, 0.7991071428571429, 0.8125, 0.796875, 0.803125, 0.8258928571428571, 0.8308823529411765, 0.8258928571428571, 0.8606770833333334, 0.8495370370370371, 0.8528225806451613, 0.8722426470588235, 0.8865131578947368, 0.8839285714285714, 0.8846726190476191, 0.8895833333333333, 0.9125, 0.9170918367346939, 0.9228316326530612, 0.9375, 0.9375, 0.9441964285714286, 0.9441964285714286, 0.9576271186440678, 0.9507415254237288, 0.9682539682539683, 0.9682539682539683, 0.9663825757575758, 0.9692234848484849, 0.9754464285714286, 0.9790178571428572, 0.978125, 0.9852678571428571, 0.9830357142857142, 0.9861607142857143, 0.9888392857142857, 0.9897321428571428, 0.9901785714285715, 0.9924107142857143, 0.9928571428571429, 0.9910714285714286, 0.9941964285714285, 0.9933035714285714, 0.9950892857142857, 0.99375, 0.9959821428571428, 0.9973214285714286, 0.9959821428571428, 0.9946428571428572, 0.9941964285714285, 0.9977678571428571, 0.9955357142857143, 0.9950892857142857, 0.9955357142857143, 0.9982142857142857, 0.9955357142857143, 0.9977678571428571, 0.9977678571428571, 0.9950892857142857, 0.9973214285714286, 0.9955357142857143, 0.9964285714285714, 0.9973214285714286, 0.9973214285714286, 0.9986607142857142, 0.9964285714285714, 0.9977678571428571, 0.9982142857142857, 1.0, 1.0, 0.9986607142857142, 0.9991071428571429, 0.9986607142857142, 0.9964285714285714, 0.9982142857142857, 0.9991071428571429, 0.9991071428571429, 1.0, 1.0, 0.9986607142857142, 1.0, 1.0, 0.9986607142857142, 0.9986607142857142, 0.9986607142857142, 0.9950892857142857]\n","Accuracy test:\n","[0.07678571428571429, 0.1375, 0.22857142857142856, 0.32321428571428573, 0.3875, 0.45535714285714285, 0.49642857142857144, 0.5410714285714285, 0.5303571428571429, 0.5732142857142857, 0.6035714285714285, 0.6178571428571429, 0.6517857142857143, 0.6553571428571429, 0.6517857142857143, 0.6875, 0.7017857142857142, 0.7107142857142857, 0.7267857142857143, 0.7267857142857143, 0.7267857142857143, 0.7535714285714286, 0.7767857142857143, 0.7892857142857143, 0.7946428571428571, 0.8089285714285714, 0.8321428571428572, 0.8357142857142857, 0.8392857142857143, 0.8446428571428571, 0.8464285714285714, 0.8553571428571428, 0.8589285714285714, 0.8589285714285714, 0.8678571428571429, 0.8660714285714286, 0.8785714285714286, 0.8821428571428571, 0.8732142857142857, 0.8821428571428571, 0.8857142857142857, 0.8821428571428571, 0.8803571428571428, 0.8785714285714286, 0.8910714285714286, 0.8964285714285715, 0.8964285714285715, 0.8821428571428571, 0.8839285714285714, 0.8946428571428572, 0.8928571428571429, 0.8946428571428572, 0.8910714285714286, 0.8964285714285715, 0.8910714285714286, 0.9, 0.9017857142857143, 0.8964285714285715, 0.9, 0.9071428571428571, 0.8964285714285715, 0.9053571428571429, 0.9017857142857143, 0.8875, 0.8928571428571429, 0.9178571428571428, 0.8964285714285715, 0.8964285714285715, 0.8910714285714286, 0.9107142857142857, 0.9, 0.8910714285714286, 0.9071428571428571, 0.9035714285714286, 0.9017857142857143, 0.9017857142857143, 0.8982142857142857, 0.9017857142857143, 0.9107142857142857, 0.9035714285714286, 0.8892857142857142, 0.9053571428571429, 0.9142857142857143, 0.9142857142857143, 0.9178571428571428, 0.9089285714285714, 0.9232142857142858, 0.9125, 0.9142857142857143, 0.9035714285714286, 0.9053571428571429, 0.9178571428571428, 0.9178571428571428, 0.9107142857142857, 0.9160714285714285, 0.9142857142857143, 0.9089285714285714, 0.9160714285714285, 0.9071428571428571, 0.8928571428571429]\n","Loss train:\n","[6.151878152574811, 4.673902000699725, 4.1179153578622, 3.201441322054182, 2.623275671686445, 2.206867814064026, 1.6504265751157488, 1.4050034029143197, 1.3540822948728288, 1.1196108034678869, 0.8046762006623405, 0.9816337057522365, 0.8910921003137316, 0.7620446256228856, 0.8483534242425647, 0.6451249548367092, 0.649955506835665, 0.6312629103660583, 0.5899797663092613, 0.5591062690530505, 0.5574728548526764, 0.5594515814667657, 0.4051692442347606, 0.48015279074509937, 0.41632580661004587, 0.39127488905454383, 0.3399363366004668, 0.3361245285542238, 0.33716768877846853, 0.30223559108045367, 0.2305424724188116, 0.23513865927044225, 0.2254272643096593, 0.1914087743140184, 0.18145268503576517, 0.17069860907005413, 0.16992703120091132, 0.13140218875418275, 0.13732972570647628, 0.10478440666246036, 0.10088525996321723, 0.10469401969263951, 0.08894416534652312, 0.09126878262364439, 0.08111559064792735, 0.07419857082755438, 0.0622631682615195, 0.06377718483230897, 0.051513651359294144, 0.043222209910995193, 0.04537192907716547, 0.040734122933021616, 0.03697455869322377, 0.0337474153842777, 0.03428591386514849, 0.027361251237536115, 0.032548319159208666, 0.023706116199692977, 0.027710325503721833, 0.024334825598634778, 0.01702686285965943, 0.017707008798606694, 0.01921138332857351, 0.02040038258003603, 0.014033713439545993, 0.019265504447477204, 0.019068722314633694, 0.018545808832693314, 0.012315693940451768, 0.018014086257400256, 0.012406993620762868, 0.01163173900318465, 0.016200686036609114, 0.01279003395881903, 0.0173376922940536, 0.012934725465519088, 0.012859338605942737, 0.010170356456157086, 0.009461541702538462, 0.015712061909393275, 0.010366672793835668, 0.008850537878710643, 0.005140394950701323, 0.004415924828832171, 0.006707080046804289, 0.0064063253226257595, 0.005918034573551267, 0.011943227803983193, 0.009182520637321952, 0.006815809699557056, 0.006838241870406949, 0.003955397409819332, 0.004144775447951231, 0.005738592830104088, 0.0039437896023238345, 0.002258185603048852, 0.007326479741979191, 0.007096532365540043, 0.006385089922280583, 0.014450305067085927]\n","Loss test:\n","[6.800798715863909, 5.730314527239118, 4.809341049194336, 4.098525020054408, 3.444785145350865, 2.9642707279750278, 2.5608219827924454, 2.262063367026193, 2.0550652844565254, 1.8577116012573243, 1.7264537504741124, 1.541912385395595, 1.4295050621032714, 1.34213901587895, 1.2973935944693429, 1.2002505949565343, 1.1208799225943429, 1.0686634540557862, 1.0103700842176164, 0.9703245077814374, 0.9076524547168187, 0.8169542959758214, 0.7688791819981167, 0.6984182826110295, 0.6636640318802425, 0.630723830631801, 0.5626833353723798, 0.5515871047973633, 0.5088219923632485, 0.47269391545227596, 0.47500282270567756, 0.4392711690493992, 0.4055068607841219, 0.42798876762390137, 0.3885721347161702, 0.4185758224555424, 0.3986843709434782, 0.36575433356421333, 0.3726407293762479, 0.38450628178460255, 0.37306890530245645, 0.3657556823321751, 0.35863484059061324, 0.3685682058334351, 0.33323478187833516, 0.34738114135605946, 0.33998103546244757, 0.35259061200278147, 0.3516291005270822, 0.33256986822400775, 0.3353597387671471, 0.3388096851961953, 0.3445593523127692, 0.342459271635328, 0.3293343714305333, 0.3585744523576328, 0.36141056844166347, 0.3304838940501213, 0.31869135115827835, 0.31875463340963633, 0.3413894849164145, 0.3447049435760294, 0.35283319162470955, 0.3708398626319, 0.44686708109719414, 0.33120712244084904, 0.3474201683487211, 0.33735748305916785, 0.3815418730889048, 0.353380511701107, 0.375917112082243, 0.3542076962334769, 0.34265059743608745, 0.3662774936429092, 0.37271460965275766, 0.36539885997772215, 0.37730582228728704, 0.36634916854756217, 0.3398062926317964, 0.36954679297549387, 0.3755026038203921, 0.36202210517866273, 0.33023077877504486, 0.3481970166521413, 0.336824143571513, 0.35249648823269775, 0.3231247602828911, 0.36262845567294527, 0.36904937656862397, 0.34088708428399905, 0.3313161514167275, 0.3454436124435493, 0.3440423502453736, 0.3260265495095934, 0.33977650552988053, 0.3197952793112823, 0.3372132469500814, 0.32481515843953407, 0.3637537028108324, 0.44751638578517094]\n","Time epoch:\n","[27.194631099700928, 48.978235721588135, 69.299152135849, 89.00983047485352, 108.88846349716187, 129.95553731918335, 150.17627477645874, 170.84940791130066, 191.35747051239014, 211.24910163879395, 231.30822920799255, 251.68375325202942, 271.5342376232147, 291.68527340888977, 312.1939580440521, 332.19310784339905, 352.2387981414795, 372.9526176452637, 393.47694635391235, 414.8144793510437, 436.48910689353943, 458.2180643081665, 480.6064569950104, 503.1675605773926, 525.9901373386383, 549.8983943462372, 573.7723135948181, 597.3932039737701, 621.4395213127136, 645.5439264774323, 670.2500681877136, 695.1419191360474, 719.9032175540924, 745.2447135448456, 770.2915744781494, 796.1275141239166, 821.4074594974518, 847.4598865509033, 873.4311518669128, 899.6883616447449, 926.3256590366364, 952.6373775005341, 979.0296337604523, 1002.1421387195587, 1025.8144266605377, 1049.179560661316, 1072.8136155605316, 1096.486162662506, 1120.0240144729614, 1143.6933705806732, 1167.490151643753, 1190.7351551055908, 1214.4121112823486, 1237.9694645404816, 1261.5374510288239, 1284.7733323574066, 1308.25466132164, 1331.9728574752808, 1355.4517529010773, 1378.7954280376434, 1402.5994884967804, 1425.8233225345612, 1449.8411235809326, 1472.9903211593628, 1497.1182165145874, 1521.1809709072113, 1544.7688837051392, 1568.547015428543, 1592.1039156913757, 1615.7112500667572, 1639.6142098903656, 1663.1134972572327, 1687.4108452796936, 1710.7462594509125, 1734.4306337833405, 1758.7664151191711, 1782.4357860088348, 1806.422468662262, 1830.4690310955048, 1856.35732960701, 1879.9782695770264, 1903.4196581840515, 1927.445959329605, 1950.9880049228668, 1974.2440702915192, 1998.1488790512085, 2021.6501269340515, 2045.6240499019623, 2069.2586572170258, 2092.8069915771484, 2118.0458314418793, 2146.9274752140045, 2173.0040652751923, 2200.9295089244843, 2227.39008808136, 2252.1585178375244, 2278.164811849594, 2304.370611190796, 2328.0550694465637, 2351.523616552353]\n","Learning rate:\n","[0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 1e-05, 1e-05, 1e-05, 1e-05, 1e-05, 1e-05, 1e-05, 1e-06, 1e-06, 1e-06, 1e-06, 1e-06, 1e-06, 1e-07, 1e-07, 1e-07, 1e-07, 1e-07, 1e-07, 1e-07, 1e-07, 1e-07, 1e-07, 1e-07, 1e-07, 1e-07, 1e-07, 1e-07, 1e-07, 1e-07, 1e-07, 1e-07, 1e-07, 1e-07, 1e-07, 1e-07, 1e-07, 1e-07, 1e-07, 1e-07, 1e-07, 1e-07, 1e-07, 1e-07, 1e-07, 1e-07, 1e-07, 1e-07, 1e-07, 1e-07, 1e-07, 1e-07, 1e-07, 1e-07, 1e-07, 1e-07, 1e-07, 1e-07, 1e-07, 1e-07, 1e-07, 1e-07, 1e-07, 1e-07, 1e-07, 1e-07, 1e-07, 1e-07, 1e-07, 1e-07, 1e-07, 1e-07, 1e-07, 1e-07]\n","Lambdas:\n","[0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.15000000000000002, 0.15000000000000002, 0.2, 0.25, 0.3, 0.35, 0.39999999999999997, 0.44999999999999996, 0.49999999999999994, 0.5499999999999999, 0.6, 0.6, 0.65, 0.65, 0.7000000000000001, 0.7000000000000001, 0.7500000000000001, 0.7500000000000001, 0.8000000000000002, 0.8000000000000002, 0.8500000000000002, 0.8500000000000002, 0.9000000000000002, 0.9000000000000002, 0.9500000000000003, 0.9500000000000003, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"]}],"source":["from torchvision.models import resnet18\n","# from ..Resnet18.task.model import ResNet18\n","import torch\n","from torch.utils.data import DataLoader, TensorDataset\n","import torch.nn as nn\n","import torch.optim as optim\n","import time\n","import random\n","\n","acc_train = []\n","acc_test = []\n","loss_train =[]\n","loss_test = []\n","time_epoch = []\n","cur_lambda = []\n","cur_learning_rate = []\n","\n","random.seed(42)\n","\n","time0 = time.time()\n","\n","data_dir = '/kaggle/input/rssnc7/RSSCN7'\n","batch_size = 32\n","learning_rate = 0.0001\n","num_epochs = 100\n","lambda_beginning = 0.1\n","lambda_end = 1\n","\n","rsscn7_data_loader = RSSCN7_DataLoader(data_dir, batch_size=batch_size, shuffle=True)\n","train_loader = rsscn7_data_loader.get_train_dataloader()\n","test_loader = rsscn7_data_loader.get_test_dataloader()\n","\n","my_device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\")\n","pretrained_model_path = \"/kaggle/input/resnet18-pretrained-on-dtd/pytorch/version1/1/resnet18_trained_on_DTD_from_80_to_90.pth\"\n","pretrained_resnet18 = ResNet18()\n","pretrained_resnet18.load_state_dict(torch.load(pretrained_model_path, map_location=torch.device(my_device)))\n","\n","model = pretrained_resnet18.to(my_device)\n","\n","model.fc = nn.Linear(47, 7)\n","\n","criterion = nn.CrossEntropyLoss()\n","opitmizer = optim.Adam(model.parameters(), lr=learning_rate)\n","\n","step = 0.05\n","\n","def train_model_self_paced(model, train_loader, test_loader, criterion, optimizer, num_epochs, learning_rate):\n","    device = my_device\n","    model.to(device)\n","    counter = 0\n","\n","    lambda_current = lambda_beginning\n","\n","    for epoch in range(num_epochs):\n","        model.train()\n","        total_loss = 0.0\n","        correct = 0\n","        total = 0\n","        train_samples = []\n","\n","        if lambda_current < 1:\n","            with torch.no_grad():\n","                for inputs, labels in train_loader:\n","                    inputs, labels = inputs.to(device), labels.to(device)\n","                    outputs = model(inputs)\n","                    loss = criterion(outputs, labels)\n","                    train_samples.append((inputs, labels, loss.item()))\n","\n","            train_samples.sort(key=lambda x: x[2])  # sort by loss (the first are the easiest)\n","\n","            num_samples_current = int(lambda_current * len(train_samples))\n","\n","            easy_enough_samples = train_samples[:num_samples_current]\n","            easy_enough_inputs = torch.cat([x[0] for x in easy_enough_samples])\n","            easy_enough_labels = torch.cat([x[1] for x in easy_enough_samples])\n","            easy_enough_dataset = TensorDataset(easy_enough_inputs, easy_enough_labels)\n","            easy_enough_loader = DataLoader(easy_enough_dataset, batch_size=batch_size, shuffle=True)\n","        else:\n","            easy_enough_loader = train_loader\n","\n","        for inputs, labels in easy_enough_loader:\n","            inputs, labels = inputs.to(device), labels.to(device)\n","\n","            optimizer.zero_grad()\n","            outputs = model(inputs)\n","            loss = criterion(outputs, labels)\n","            loss.backward()\n","            optimizer.step()\n","\n","            total_loss += loss.item() * inputs.size(0)\n","\n","            _, predicted = torch.max(outputs, 1)\n","            total += labels.size(0)\n","            correct += (predicted == labels).sum().item()\n","\n","        train_loss = total_loss / len(easy_enough_loader.dataset)\n","        train_accuracy = correct / total\n","        num_images = len(easy_enough_loader.dataset)\n","\n","        if train_accuracy >= 0.88:\n","            learning_rate = 0.00001\n","        \n","        if train_accuracy >= 0.93:\n","            learning_rate = 0.000001\n","            \n","        if train_accuracy >= 0.96:\n","            learning_rate = 0.0000001\n","\n","        cur_time_ = time.time() - time0\n","\n","        print(\"learing_rate = \", learning_rate)\n","\n","        print(\n","            f'Epoch [{epoch + 1}/{num_epochs}], Loss: {train_loss:.4f}, Accuracy: {train_accuracy:.4f}, Images: {num_images}, Lambda: {lambda_current:.2f}, Time: {cur_time_:.2f} seconds')\n","\n","        if train_accuracy > 0.8:\n","            if lambda_current < 0.6:\n","                lambda_current += step\n","                if lambda_current>1:\n","                    lambda_current = 1\n","            else:\n","                counter = counter + 1\n","                if counter % 2 == 0:\n","                    lambda_current += step\n","                    counter = 0\n","                    if lambda_current>1:\n","                        lambda_current = 1\n","\n","        acc_train.append(train_accuracy)\n","        loss_train.append(train_loss)\n","        time_epoch.append(cur_time_)\n","        cur_lambda.append(lambda_current)\n","        cur_learning_rate.append(learning_rate)\n","\n","        evaluate_model(model, test_loader, criterion)\n","\n","    print('Finished Training Successfully')\n","\n","\n","def evaluate_model(model, test_loader, criterion):\n","    model.eval()\n","    device = next(model.parameters()).device\n","    total_loss = 0.0\n","    correct = 0\n","    total = 0\n","\n","    with torch.no_grad():\n","        for inputs, labels in test_loader:\n","            inputs, labels = inputs.to(device), labels.to(device)\n","            outputs = model(inputs)\n","            loss = criterion(outputs, labels)\n","            total_loss += loss.item() * inputs.size(0)\n","\n","            _, predicted = torch.max(outputs, 1)\n","            total += labels.size(0)\n","            correct += (predicted == labels).sum().item()\n","\n","    test_loss = total_loss / len(test_loader.dataset)\n","    test_accuracy = correct / total\n","\n","    print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}')\n","\n","    acc_test.append(test_accuracy)\n","    loss_test.append(test_loss)\n","\n","\n","train_model_self_paced(model, train_loader, test_loader, criterion, opitmizer, num_epochs, learning_rate)\n","print(\"Accuracy train:\")\n","print(acc_train)\n","print(\"Accuracy test:\")\n","print(acc_test)\n","print(\"Loss train:\")\n","print(loss_train)\n","print(\"Loss test:\")\n","print(loss_test)\n","print(\"Time epoch:\")\n","print(time_epoch)\n","print(\"Learning rate:\")\n","print(cur_learning_rate)\n","print(\"Lambdas:\")\n","print(cur_lambda)\n","\n","torch.save(model, 'resnet18_DTD_self_paced.pth')"]},{"cell_type":"markdown","metadata":{},"source":["Transfer_learning_imagenet"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-05-28T10:11:01.365617Z","iopub.status.busy":"2024-05-28T10:11:01.365253Z","iopub.status.idle":"2024-05-28T10:43:14.681966Z","shell.execute_reply":"2024-05-28T10:43:14.681002Z","shell.execute_reply.started":"2024-05-28T10:11:01.365587Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n","100%|██████████| 44.7M/44.7M [00:00<00:00, 131MB/s] \n"]},{"name":"stdout","output_type":"stream","text":["learing_rate =  0.001\n","Training - Epoch 1/100, Loss: 1.2875, Accuracy: 53.5714%\n","Testing - Epoch 1/100, Loss: 0.8389, Accuracy: 77.1429%\n","Time: 24.94 seconds\n","learing_rate =  0.001\n","Training - Epoch 2/100, Loss: 0.7318, Accuracy: 78.4821%\n","Testing - Epoch 2/100, Loss: 0.6322, Accuracy: 79.8214%\n","Time: 46.18 seconds\n","learing_rate =  0.001\n","Training - Epoch 3/100, Loss: 0.5861, Accuracy: 81.8304%\n","Testing - Epoch 3/100, Loss: 0.5543, Accuracy: 81.9643%\n","Time: 65.09 seconds\n","learing_rate =  0.001\n","Training - Epoch 4/100, Loss: 0.5141, Accuracy: 83.5714%\n","Testing - Epoch 4/100, Loss: 0.4972, Accuracy: 82.5000%\n","Time: 83.97 seconds\n","learing_rate =  0.001\n","Training - Epoch 5/100, Loss: 0.4910, Accuracy: 83.9732%\n","Testing - Epoch 5/100, Loss: 0.4802, Accuracy: 83.7500%\n","Time: 103.52 seconds\n","learing_rate =  0.001\n","Training - Epoch 6/100, Loss: 0.4461, Accuracy: 84.9107%\n","Testing - Epoch 6/100, Loss: 0.4481, Accuracy: 85.0000%\n","Time: 122.58 seconds\n","learing_rate =  0.0005\n","Training - Epoch 7/100, Loss: 0.4079, Accuracy: 86.5625%\n","Testing - Epoch 7/100, Loss: 0.4563, Accuracy: 85.0000%\n","Time: 141.34 seconds\n","learing_rate =  0.0005\n","Training - Epoch 8/100, Loss: 0.3858, Accuracy: 87.8571%\n","Testing - Epoch 8/100, Loss: 0.4374, Accuracy: 83.9286%\n","Time: 160.85 seconds\n","learing_rate =  0.0005\n","Training - Epoch 9/100, Loss: 0.3823, Accuracy: 87.4107%\n","Testing - Epoch 9/100, Loss: 0.4367, Accuracy: 84.6429%\n","Time: 179.69 seconds\n","learing_rate =  0.0005\n","Training - Epoch 10/100, Loss: 0.3506, Accuracy: 88.9732%\n","Testing - Epoch 10/100, Loss: 0.4332, Accuracy: 85.8929%\n","Time: 198.76 seconds\n","learing_rate =  0.0005\n","Training - Epoch 11/100, Loss: 0.3454, Accuracy: 88.5268%\n","Testing - Epoch 11/100, Loss: 0.3933, Accuracy: 85.5357%\n","Time: 218.28 seconds\n","learing_rate =  0.0005\n","Training - Epoch 12/100, Loss: 0.3478, Accuracy: 88.4375%\n","Testing - Epoch 12/100, Loss: 0.3992, Accuracy: 85.7143%\n","Time: 237.44 seconds\n","learing_rate =  0.0005\n","Training - Epoch 13/100, Loss: 0.3324, Accuracy: 88.3929%\n","Testing - Epoch 13/100, Loss: 0.4262, Accuracy: 85.3571%\n","Time: 256.83 seconds\n","learing_rate =  0.0005\n","Training - Epoch 14/100, Loss: 0.3306, Accuracy: 88.5268%\n","Testing - Epoch 14/100, Loss: 0.4030, Accuracy: 86.2500%\n","Time: 275.95 seconds\n","learing_rate =  5e-05\n","Training - Epoch 15/100, Loss: 0.3057, Accuracy: 89.5536%\n","Testing - Epoch 15/100, Loss: 0.3821, Accuracy: 85.3571%\n","Time: 295.32 seconds\n","learing_rate =  5e-05\n","Training - Epoch 16/100, Loss: 0.3043, Accuracy: 89.2857%\n","Testing - Epoch 16/100, Loss: 0.3872, Accuracy: 87.3214%\n","Time: 314.57 seconds\n","learing_rate =  5e-05\n","Training - Epoch 17/100, Loss: 0.3072, Accuracy: 89.8214%\n","Testing - Epoch 17/100, Loss: 0.3854, Accuracy: 86.2500%\n","Time: 334.04 seconds\n","learing_rate =  5e-05\n","Training - Epoch 18/100, Loss: 0.2838, Accuracy: 90.7589%\n","Testing - Epoch 18/100, Loss: 0.3819, Accuracy: 85.8929%\n","Time: 353.73 seconds\n","learing_rate =  5e-05\n","Training - Epoch 19/100, Loss: 0.2837, Accuracy: 90.5357%\n","Testing - Epoch 19/100, Loss: 0.4085, Accuracy: 85.5357%\n","Time: 373.11 seconds\n","learing_rate =  5e-05\n","Training - Epoch 20/100, Loss: 0.2778, Accuracy: 90.3571%\n","Testing - Epoch 20/100, Loss: 0.3845, Accuracy: 85.1786%\n","Time: 392.46 seconds\n","learing_rate =  5e-05\n","Training - Epoch 21/100, Loss: 0.2710, Accuracy: 91.1161%\n","Testing - Epoch 21/100, Loss: 0.3712, Accuracy: 86.2500%\n","Time: 411.79 seconds\n","learing_rate =  5e-05\n","Training - Epoch 22/100, Loss: 0.2668, Accuracy: 90.8929%\n","Testing - Epoch 22/100, Loss: 0.3691, Accuracy: 87.3214%\n","Time: 430.58 seconds\n","learing_rate =  5e-05\n","Training - Epoch 23/100, Loss: 0.2794, Accuracy: 89.3750%\n","Testing - Epoch 23/100, Loss: 0.3794, Accuracy: 86.0714%\n","Time: 449.85 seconds\n","learing_rate =  5e-05\n","Training - Epoch 24/100, Loss: 0.2489, Accuracy: 91.3393%\n","Testing - Epoch 24/100, Loss: 0.3696, Accuracy: 86.7857%\n","Time: 469.41 seconds\n","learing_rate =  5e-05\n","Training - Epoch 25/100, Loss: 0.2494, Accuracy: 91.2054%\n","Testing - Epoch 25/100, Loss: 0.3597, Accuracy: 86.9643%\n","Time: 488.75 seconds\n","learing_rate =  5e-05\n","Training - Epoch 26/100, Loss: 0.2534, Accuracy: 90.8929%\n","Testing - Epoch 26/100, Loss: 0.3902, Accuracy: 86.7857%\n","Time: 508.73 seconds\n","learing_rate =  5e-05\n","Training - Epoch 27/100, Loss: 0.2403, Accuracy: 91.8304%\n","Testing - Epoch 27/100, Loss: 0.3867, Accuracy: 87.1429%\n","Time: 528.14 seconds\n","learing_rate =  5e-05\n","Training - Epoch 28/100, Loss: 0.2404, Accuracy: 91.5179%\n","Testing - Epoch 28/100, Loss: 0.3815, Accuracy: 85.7143%\n","Time: 547.28 seconds\n","learing_rate =  1e-06\n","Training - Epoch 29/100, Loss: 0.2289, Accuracy: 92.5446%\n","Testing - Epoch 29/100, Loss: 0.3750, Accuracy: 86.7857%\n","Time: 566.31 seconds\n","learing_rate =  1e-06\n","Training - Epoch 30/100, Loss: 0.2228, Accuracy: 92.6786%\n","Testing - Epoch 30/100, Loss: 0.3731, Accuracy: 87.6786%\n","Time: 585.83 seconds\n","learing_rate =  5e-05\n","Training - Epoch 31/100, Loss: 0.2401, Accuracy: 91.4286%\n","Testing - Epoch 31/100, Loss: 0.4106, Accuracy: 87.5000%\n","Time: 605.20 seconds\n","learing_rate =  1e-06\n","Training - Epoch 32/100, Loss: 0.2249, Accuracy: 92.3214%\n","Testing - Epoch 32/100, Loss: 0.3740, Accuracy: 86.7857%\n","Time: 624.40 seconds\n","learing_rate =  1e-06\n","Training - Epoch 33/100, Loss: 0.2154, Accuracy: 92.2321%\n","Testing - Epoch 33/100, Loss: 0.4073, Accuracy: 85.7143%\n","Time: 643.72 seconds\n","learing_rate =  1e-06\n","Training - Epoch 34/100, Loss: 0.2219, Accuracy: 92.5893%\n","Testing - Epoch 34/100, Loss: 0.3733, Accuracy: 87.1429%\n","Time: 663.06 seconds\n","learing_rate =  1e-06\n","Training - Epoch 35/100, Loss: 0.2143, Accuracy: 92.6786%\n","Testing - Epoch 35/100, Loss: 0.3663, Accuracy: 86.9643%\n","Time: 681.95 seconds\n","learing_rate =  1e-06\n","Training - Epoch 36/100, Loss: 0.2232, Accuracy: 92.0089%\n","Testing - Epoch 36/100, Loss: 0.3667, Accuracy: 87.3214%\n","Time: 701.46 seconds\n","learing_rate =  1e-06\n","Training - Epoch 37/100, Loss: 0.1952, Accuracy: 93.6161%\n","Testing - Epoch 37/100, Loss: 0.4065, Accuracy: 86.2500%\n","Time: 720.51 seconds\n","learing_rate =  1e-06\n","Training - Epoch 38/100, Loss: 0.2067, Accuracy: 92.8571%\n","Testing - Epoch 38/100, Loss: 0.3857, Accuracy: 86.4286%\n","Time: 739.41 seconds\n","learing_rate =  1e-06\n","Training - Epoch 39/100, Loss: 0.2096, Accuracy: 92.4554%\n","Testing - Epoch 39/100, Loss: 0.3906, Accuracy: 87.6786%\n","Time: 759.10 seconds\n","learing_rate =  1e-06\n","Training - Epoch 40/100, Loss: 0.1974, Accuracy: 93.2143%\n","Testing - Epoch 40/100, Loss: 0.3776, Accuracy: 87.6786%\n","Time: 778.32 seconds\n","learing_rate =  1e-06\n","Training - Epoch 41/100, Loss: 0.2002, Accuracy: 93.0357%\n","Testing - Epoch 41/100, Loss: 0.3673, Accuracy: 87.1429%\n","Time: 797.20 seconds\n","learing_rate =  1e-06\n","Training - Epoch 42/100, Loss: 0.2245, Accuracy: 92.0536%\n","Testing - Epoch 42/100, Loss: 0.3967, Accuracy: 86.6071%\n","Time: 816.51 seconds\n","learing_rate =  1e-06\n","Training - Epoch 43/100, Loss: 0.1956, Accuracy: 92.9464%\n","Testing - Epoch 43/100, Loss: 0.3952, Accuracy: 86.9643%\n","Time: 835.29 seconds\n","learing_rate =  1e-06\n","Training - Epoch 44/100, Loss: 0.2123, Accuracy: 92.6786%\n","Testing - Epoch 44/100, Loss: 0.3714, Accuracy: 87.3214%\n","Time: 854.16 seconds\n","learing_rate =  1e-06\n","Training - Epoch 45/100, Loss: 0.1906, Accuracy: 93.7054%\n","Testing - Epoch 45/100, Loss: 0.3868, Accuracy: 86.6071%\n","Time: 873.27 seconds\n","learing_rate =  1e-06\n","Training - Epoch 46/100, Loss: 0.2071, Accuracy: 92.4554%\n","Testing - Epoch 46/100, Loss: 0.3759, Accuracy: 87.5000%\n","Time: 892.60 seconds\n","learing_rate =  1e-06\n","Training - Epoch 47/100, Loss: 0.2016, Accuracy: 93.3929%\n","Testing - Epoch 47/100, Loss: 0.3703, Accuracy: 86.2500%\n","Time: 911.58 seconds\n","learing_rate =  1e-06\n","Training - Epoch 48/100, Loss: 0.1852, Accuracy: 93.5268%\n","Testing - Epoch 48/100, Loss: 0.4105, Accuracy: 86.6071%\n","Time: 930.65 seconds\n","learing_rate =  1e-06\n","Training - Epoch 49/100, Loss: 0.1862, Accuracy: 93.4821%\n","Testing - Epoch 49/100, Loss: 0.4197, Accuracy: 85.8929%\n","Time: 950.44 seconds\n","learing_rate =  1e-06\n","Training - Epoch 50/100, Loss: 0.1688, Accuracy: 94.1071%\n","Testing - Epoch 50/100, Loss: 0.3932, Accuracy: 87.1429%\n","Time: 969.51 seconds\n","learing_rate =  1e-06\n","Training - Epoch 51/100, Loss: 0.1682, Accuracy: 94.5536%\n","Testing - Epoch 51/100, Loss: 0.3726, Accuracy: 87.6786%\n","Time: 988.70 seconds\n","learing_rate =  1e-06\n","Training - Epoch 52/100, Loss: 0.1781, Accuracy: 93.8839%\n","Testing - Epoch 52/100, Loss: 0.3913, Accuracy: 86.6071%\n","Time: 1008.09 seconds\n","learing_rate =  1e-06\n","Training - Epoch 53/100, Loss: 0.1776, Accuracy: 94.1518%\n","Testing - Epoch 53/100, Loss: 0.3903, Accuracy: 86.9643%\n","Time: 1026.95 seconds\n","learing_rate =  1e-06\n","Training - Epoch 54/100, Loss: 0.1682, Accuracy: 94.8214%\n","Testing - Epoch 54/100, Loss: 0.4038, Accuracy: 86.2500%\n","Time: 1045.88 seconds\n","learing_rate =  1e-06\n","Training - Epoch 55/100, Loss: 0.1801, Accuracy: 94.1518%\n","Testing - Epoch 55/100, Loss: 0.3937, Accuracy: 88.0357%\n","Time: 1065.67 seconds\n","learing_rate =  1e-06\n","Training - Epoch 56/100, Loss: 0.1737, Accuracy: 94.6875%\n","Testing - Epoch 56/100, Loss: 0.3760, Accuracy: 88.2143%\n","Time: 1084.46 seconds\n","learing_rate =  1e-06\n","Training - Epoch 57/100, Loss: 0.1688, Accuracy: 94.6429%\n","Testing - Epoch 57/100, Loss: 0.3969, Accuracy: 86.4286%\n","Time: 1103.42 seconds\n","learing_rate =  1e-06\n","Training - Epoch 58/100, Loss: 0.1936, Accuracy: 93.0804%\n","Testing - Epoch 58/100, Loss: 0.4070, Accuracy: 87.5000%\n","Time: 1122.82 seconds\n","learing_rate =  1e-06\n","Training - Epoch 59/100, Loss: 0.1846, Accuracy: 93.7054%\n","Testing - Epoch 59/100, Loss: 0.3801, Accuracy: 88.2143%\n","Time: 1141.99 seconds\n","learing_rate =  1e-06\n","Training - Epoch 60/100, Loss: 0.1552, Accuracy: 95.1339%\n","Testing - Epoch 60/100, Loss: 0.3744, Accuracy: 87.5000%\n","Time: 1161.02 seconds\n","learing_rate =  1e-06\n","Training - Epoch 61/100, Loss: 0.1621, Accuracy: 93.7946%\n","Testing - Epoch 61/100, Loss: 0.3867, Accuracy: 87.8571%\n","Time: 1180.19 seconds\n","learing_rate =  1e-06\n","Training - Epoch 62/100, Loss: 0.1739, Accuracy: 93.4821%\n","Testing - Epoch 62/100, Loss: 0.3816, Accuracy: 85.8929%\n","Time: 1199.36 seconds\n","learing_rate =  1e-06\n","Training - Epoch 63/100, Loss: 0.1599, Accuracy: 94.9107%\n","Testing - Epoch 63/100, Loss: 0.4208, Accuracy: 86.9643%\n","Time: 1218.70 seconds\n","learing_rate =  1e-06\n","Training - Epoch 64/100, Loss: 0.1604, Accuracy: 94.6875%\n","Testing - Epoch 64/100, Loss: 0.3779, Accuracy: 87.6786%\n","Time: 1238.86 seconds\n","learing_rate =  1e-06\n","Training - Epoch 65/100, Loss: 0.1548, Accuracy: 94.8214%\n","Testing - Epoch 65/100, Loss: 0.3913, Accuracy: 86.9643%\n","Time: 1258.03 seconds\n","learing_rate =  1e-06\n","Training - Epoch 66/100, Loss: 0.1496, Accuracy: 95.0446%\n","Testing - Epoch 66/100, Loss: 0.4043, Accuracy: 86.9643%\n","Time: 1276.87 seconds\n","learing_rate =  1e-06\n","Training - Epoch 67/100, Loss: 0.1584, Accuracy: 94.9107%\n","Testing - Epoch 67/100, Loss: 0.3991, Accuracy: 87.1429%\n","Time: 1296.29 seconds\n","learing_rate =  1e-06\n","Training - Epoch 68/100, Loss: 0.1558, Accuracy: 94.9554%\n","Testing - Epoch 68/100, Loss: 0.4076, Accuracy: 86.7857%\n","Time: 1315.25 seconds\n","learing_rate =  1e-06\n","Training - Epoch 69/100, Loss: 0.1608, Accuracy: 94.8661%\n","Testing - Epoch 69/100, Loss: 0.4087, Accuracy: 87.1429%\n","Time: 1334.23 seconds\n","learing_rate =  1e-06\n","Training - Epoch 70/100, Loss: 0.1556, Accuracy: 94.5982%\n","Testing - Epoch 70/100, Loss: 0.3919, Accuracy: 87.5000%\n","Time: 1353.30 seconds\n","learing_rate =  1e-06\n","Training - Epoch 71/100, Loss: 0.1401, Accuracy: 95.3125%\n","Testing - Epoch 71/100, Loss: 0.4110, Accuracy: 86.7857%\n","Time: 1372.49 seconds\n","learing_rate =  1e-06\n","Training - Epoch 72/100, Loss: 0.1523, Accuracy: 94.7768%\n","Testing - Epoch 72/100, Loss: 0.3917, Accuracy: 87.1429%\n","Time: 1391.44 seconds\n","learing_rate =  1e-06\n","Training - Epoch 73/100, Loss: 0.1570, Accuracy: 94.0625%\n","Testing - Epoch 73/100, Loss: 0.4207, Accuracy: 86.7857%\n","Time: 1410.30 seconds\n","learing_rate =  1e-06\n","Training - Epoch 74/100, Loss: 0.1448, Accuracy: 95.0893%\n","Testing - Epoch 74/100, Loss: 0.4047, Accuracy: 86.6071%\n","Time: 1429.56 seconds\n","learing_rate =  1e-06\n","Training - Epoch 75/100, Loss: 0.1430, Accuracy: 94.8661%\n","Testing - Epoch 75/100, Loss: 0.4150, Accuracy: 86.7857%\n","Time: 1448.66 seconds\n","learing_rate =  1e-06\n","Training - Epoch 76/100, Loss: 0.1640, Accuracy: 94.4196%\n","Testing - Epoch 76/100, Loss: 0.4312, Accuracy: 85.7143%\n","Time: 1467.61 seconds\n","learing_rate =  1e-06\n","Training - Epoch 77/100, Loss: 0.1510, Accuracy: 95.2232%\n","Testing - Epoch 77/100, Loss: 0.3994, Accuracy: 86.7857%\n","Time: 1486.91 seconds\n","learing_rate =  1e-06\n","Training - Epoch 78/100, Loss: 0.1446, Accuracy: 95.0000%\n","Testing - Epoch 78/100, Loss: 0.4093, Accuracy: 86.6071%\n","Time: 1505.88 seconds\n","learing_rate =  1e-06\n","Training - Epoch 79/100, Loss: 0.1463, Accuracy: 94.6429%\n","Testing - Epoch 79/100, Loss: 0.4083, Accuracy: 87.3214%\n","Time: 1524.78 seconds\n","learing_rate =  1e-06\n","Training - Epoch 80/100, Loss: 0.1395, Accuracy: 95.7143%\n","Testing - Epoch 80/100, Loss: 0.4151, Accuracy: 87.1429%\n","Time: 1544.96 seconds\n","learing_rate =  1e-06\n","Training - Epoch 81/100, Loss: 0.1435, Accuracy: 95.2232%\n","Testing - Epoch 81/100, Loss: 0.4021, Accuracy: 86.4286%\n","Time: 1563.96 seconds\n","learing_rate =  1e-06\n","Training - Epoch 82/100, Loss: 0.1425, Accuracy: 95.4018%\n","Testing - Epoch 82/100, Loss: 0.4092, Accuracy: 86.4286%\n","Time: 1583.04 seconds\n","learing_rate =  1e-06\n","Training - Epoch 83/100, Loss: 0.1393, Accuracy: 95.0893%\n","Testing - Epoch 83/100, Loss: 0.4108, Accuracy: 87.5000%\n","Time: 1602.41 seconds\n","learing_rate =  1e-06\n","Training - Epoch 84/100, Loss: 0.1540, Accuracy: 94.7768%\n","Testing - Epoch 84/100, Loss: 0.4221, Accuracy: 86.2500%\n","Time: 1621.26 seconds\n","learing_rate =  1e-06\n","Training - Epoch 85/100, Loss: 0.1313, Accuracy: 95.4911%\n","Testing - Epoch 85/100, Loss: 0.4210, Accuracy: 87.5000%\n","Time: 1640.36 seconds\n","learing_rate =  1e-06\n","Training - Epoch 86/100, Loss: 0.1463, Accuracy: 94.3304%\n","Testing - Epoch 86/100, Loss: 0.4101, Accuracy: 86.6071%\n","Time: 1659.47 seconds\n","learing_rate =  1e-06\n","Training - Epoch 87/100, Loss: 0.1356, Accuracy: 94.9107%\n","Testing - Epoch 87/100, Loss: 0.4336, Accuracy: 87.1429%\n","Time: 1678.72 seconds\n","learing_rate =  1e-06\n","Training - Epoch 88/100, Loss: 0.1360, Accuracy: 96.0268%\n","Testing - Epoch 88/100, Loss: 0.4208, Accuracy: 87.8571%\n","Time: 1697.92 seconds\n","learing_rate =  1e-06\n","Training - Epoch 89/100, Loss: 0.1430, Accuracy: 95.0000%\n","Testing - Epoch 89/100, Loss: 0.4330, Accuracy: 86.6071%\n","Time: 1717.35 seconds\n","learing_rate =  1e-06\n","Training - Epoch 90/100, Loss: 0.1466, Accuracy: 94.8661%\n","Testing - Epoch 90/100, Loss: 0.4242, Accuracy: 87.6786%\n","Time: 1737.04 seconds\n","learing_rate =  1e-06\n","Training - Epoch 91/100, Loss: 0.1432, Accuracy: 95.0893%\n","Testing - Epoch 91/100, Loss: 0.4111, Accuracy: 86.7857%\n","Time: 1755.96 seconds\n","learing_rate =  1e-06\n","Training - Epoch 92/100, Loss: 0.1348, Accuracy: 95.7143%\n","Testing - Epoch 92/100, Loss: 0.4071, Accuracy: 87.1429%\n","Time: 1775.35 seconds\n","learing_rate =  1e-06\n","Training - Epoch 93/100, Loss: 0.1374, Accuracy: 95.3125%\n","Testing - Epoch 93/100, Loss: 0.4197, Accuracy: 87.6786%\n","Time: 1794.76 seconds\n","learing_rate =  1e-06\n","Training - Epoch 94/100, Loss: 0.1210, Accuracy: 96.0268%\n","Testing - Epoch 94/100, Loss: 0.4181, Accuracy: 86.7857%\n","Time: 1813.60 seconds\n","learing_rate =  1e-06\n","Training - Epoch 95/100, Loss: 0.1391, Accuracy: 95.0446%\n","Testing - Epoch 95/100, Loss: 0.4375, Accuracy: 86.4286%\n","Time: 1832.93 seconds\n","learing_rate =  1e-06\n","Training - Epoch 96/100, Loss: 0.1282, Accuracy: 95.4911%\n","Testing - Epoch 96/100, Loss: 0.4234, Accuracy: 86.2500%\n","Time: 1852.52 seconds\n","learing_rate =  1e-06\n","Training - Epoch 97/100, Loss: 0.1507, Accuracy: 94.1964%\n","Testing - Epoch 97/100, Loss: 0.4624, Accuracy: 85.7143%\n","Time: 1871.65 seconds\n","learing_rate =  1e-06\n","Training - Epoch 98/100, Loss: 0.1587, Accuracy: 94.7768%\n","Testing - Epoch 98/100, Loss: 0.4115, Accuracy: 87.8571%\n","Time: 1891.11 seconds\n","learing_rate =  1e-06\n","Training - Epoch 99/100, Loss: 0.1321, Accuracy: 95.4018%\n","Testing - Epoch 99/100, Loss: 0.4406, Accuracy: 86.2500%\n","Time: 1910.26 seconds\n","learing_rate =  1e-06\n","Training - Epoch 100/100, Loss: 0.1366, Accuracy: 95.0446%\n","Testing - Epoch 100/100, Loss: 0.4373, Accuracy: 86.7857%\n","Time: 1929.38 seconds\n","Training complete.\n","Training completed successfully.\n","Training accuracy:\n","[53.57142857142857, 78.48214285714286, 81.83035714285715, 83.57142857142857, 83.97321428571428, 84.91071428571428, 86.5625, 87.85714285714286, 87.41071428571429, 88.97321428571429, 88.52678571428572, 88.4375, 88.39285714285714, 88.52678571428572, 89.55357142857143, 89.28571428571429, 89.82142857142857, 90.75892857142858, 90.53571428571429, 90.35714285714286, 91.11607142857143, 90.89285714285714, 89.375, 91.33928571428571, 91.20535714285715, 90.89285714285714, 91.83035714285714, 91.51785714285714, 92.54464285714286, 92.67857142857143, 91.42857142857143, 92.32142857142858, 92.23214285714286, 92.58928571428572, 92.67857142857143, 92.00892857142857, 93.61607142857142, 92.85714285714286, 92.45535714285714, 93.21428571428572, 93.03571428571429, 92.05357142857142, 92.94642857142857, 92.67857142857143, 93.70535714285714, 92.45535714285714, 93.39285714285714, 93.52678571428571, 93.48214285714286, 94.10714285714286, 94.55357142857143, 93.88392857142858, 94.15178571428572, 94.82142857142857, 94.15178571428572, 94.6875, 94.64285714285714, 93.08035714285714, 93.70535714285714, 95.13392857142857, 93.79464285714286, 93.48214285714286, 94.91071428571428, 94.6875, 94.82142857142857, 95.04464285714286, 94.91071428571428, 94.95535714285714, 94.86607142857143, 94.59821428571429, 95.3125, 94.77678571428572, 94.0625, 95.08928571428571, 94.86607142857143, 94.41964285714286, 95.22321428571429, 95.0, 94.64285714285714, 95.71428571428572, 95.22321428571429, 95.40178571428571, 95.08928571428571, 94.77678571428572, 95.49107142857143, 94.33035714285715, 94.91071428571428, 96.02678571428571, 95.0, 94.86607142857143, 95.08928571428571, 95.71428571428572, 95.3125, 96.02678571428571, 95.04464285714286, 95.49107142857143, 94.19642857142857, 94.77678571428572, 95.40178571428571, 95.04464285714286]\n","Test accuracy:\n","[77.14285714285715, 79.82142857142858, 81.96428571428571, 82.5, 83.75, 85.0, 85.0, 83.92857142857143, 84.64285714285714, 85.89285714285714, 85.53571428571428, 85.71428571428571, 85.35714285714285, 86.25, 85.35714285714285, 87.32142857142857, 86.25, 85.89285714285714, 85.53571428571428, 85.17857142857143, 86.25, 87.32142857142857, 86.07142857142858, 86.78571428571429, 86.96428571428572, 86.78571428571429, 87.14285714285714, 85.71428571428571, 86.78571428571429, 87.67857142857143, 87.5, 86.78571428571429, 85.71428571428571, 87.14285714285714, 86.96428571428572, 87.32142857142857, 86.25, 86.42857142857143, 87.67857142857143, 87.67857142857143, 87.14285714285714, 86.60714285714286, 86.96428571428572, 87.32142857142857, 86.60714285714286, 87.5, 86.25, 86.60714285714286, 85.89285714285714, 87.14285714285714, 87.67857142857143, 86.60714285714286, 86.96428571428572, 86.25, 88.03571428571428, 88.21428571428571, 86.42857142857143, 87.5, 88.21428571428571, 87.5, 87.85714285714286, 85.89285714285714, 86.96428571428572, 87.67857142857143, 86.96428571428572, 86.96428571428572, 87.14285714285714, 86.78571428571429, 87.14285714285714, 87.5, 86.78571428571429, 87.14285714285714, 86.78571428571429, 86.60714285714286, 86.78571428571429, 85.71428571428571, 86.78571428571429, 86.60714285714286, 87.32142857142857, 87.14285714285714, 86.42857142857143, 86.42857142857143, 87.5, 86.25, 87.5, 86.60714285714286, 87.14285714285714, 87.85714285714286, 86.60714285714286, 87.67857142857143, 86.78571428571429, 87.14285714285714, 87.67857142857143, 86.78571428571429, 86.42857142857143, 86.25, 85.71428571428571, 87.85714285714286, 86.25, 86.78571428571429]\n","Loss train:\n","[1.287484007222312, 0.7318366519042424, 0.5860984244516918, 0.514121567777225, 0.49100095459393095, 0.44607578963041306, 0.40793614621673313, 0.38576409710305076, 0.3823499413473265, 0.35059218896286826, 0.3453617532338415, 0.34779987846102034, 0.33239436617919377, 0.3306440230991159, 0.3056919398052352, 0.3042594097554684, 0.3072024398616382, 0.28384869088019643, 0.2836738616228104, 0.2777570733002254, 0.2710468256047794, 0.2667860312121255, 0.27935574278235437, 0.24891781200255667, 0.24943878299423627, 0.2534427237297807, 0.2403123239321368, 0.24042135168399129, 0.22893917933106422, 0.22282316477171013, 0.24007867393749102, 0.22493692934513093, 0.21536013010357108, 0.22191328534058163, 0.2143040472375495, 0.22321888080665042, 0.19521556943655013, 0.2066797855177096, 0.2096439101334129, 0.19739548532026155, 0.20018557459115982, 0.22445751705339978, 0.19559149625045913, 0.21233340267624173, 0.19060046396085195, 0.20705053423132216, 0.20161713542682783, 0.18517930417188577, 0.18623013155800955, 0.16875024203743252, 0.16815841884485314, 0.17807447516492436, 0.17758222350052424, 0.16816791466304234, 0.18007400397743498, 0.17367498049778599, 0.1688493794628552, 0.19355755815548556, 0.18460115114493028, 0.15524944085627795, 0.16205337281738008, 0.17389104046991893, 0.15992655365594796, 0.16035451548440116, 0.15482369980641775, 0.14964854062667915, 0.15844565284039294, 0.15582991516483682, 0.16081859235252652, 0.1556428184998887, 0.1400759286646332, 0.1522650527634791, 0.15701650444950377, 0.14479007832705976, 0.14303089984293496, 0.16403885462454387, 0.15104938947728702, 0.14458304159343244, 0.14628448092511723, 0.13951764165290764, 0.1435348049338375, 0.14250495604106359, 0.13926464090389865, 0.1539994499246989, 0.13133712562599353, 0.1462529272905418, 0.13562201524951628, 0.1359861885862691, 0.14303870733295168, 0.1465614148016487, 0.14316353936280524, 0.1347640575574977, 0.13742997872510127, 0.12101086984787668, 0.13907199933060577, 0.1282279545707362, 0.15067516935190983, 0.15872222456548896, 0.13211933393031358, 0.1365971568173596]\n","Test loss:\n","[0.8388757722718375, 0.6322453081607818, 0.5542962482997349, 0.49718623076166424, 0.4801685401371547, 0.44807338884898595, 0.45634634239333016, 0.4373535326548985, 0.4366874498980386, 0.4332016374383654, 0.39326767240251814, 0.3991562383515494, 0.42624599082129344, 0.40300399661064146, 0.38210871304784505, 0.3871648933206286, 0.38539302008492604, 0.3819097306047167, 0.4085285748754229, 0.38452579975128176, 0.37115761467388697, 0.3691376507282257, 0.37941879885537283, 0.3696360102721623, 0.3597383754593985, 0.3901608850274767, 0.38667237077440536, 0.3814918126378741, 0.37502271022115435, 0.37314854179109846, 0.4105600144181933, 0.3740399752344404, 0.4072576914514814, 0.37331720335142954, 0.36631326590265545, 0.3666932787214007, 0.4065188901765006, 0.38567331433296204, 0.39055449536868503, 0.37762001412255425, 0.3673375632081713, 0.39669992072241644, 0.39519363982336864, 0.3713759158338819, 0.3868222841194698, 0.3759462484291622, 0.3703400841781071, 0.4105026406901223, 0.4196566241128104, 0.3932006061077118, 0.37262403454099385, 0.39125136988503595, 0.39033998250961305, 0.40377021091324944, 0.39368743896484376, 0.37601631283760073, 0.3969074879373823, 0.4069818879876818, 0.3800682170050485, 0.37436001215662273, 0.3867107936314174, 0.3816303185054234, 0.42075455699648173, 0.3779185090746198, 0.3913040322916848, 0.40426267215183803, 0.39907962509563993, 0.4076308957168034, 0.40869713340486796, 0.3918667784758976, 0.41096208095550535, 0.3916992153440203, 0.4207197640623365, 0.4047055797917502, 0.41496032902172636, 0.43123098526682174, 0.39938457693372453, 0.40931820443698336, 0.40832227638789587, 0.4150680822985513, 0.40212317194257463, 0.40922052689961025, 0.4107565641403198, 0.4220610499382019, 0.42098416260310584, 0.4101213080542428, 0.43356857129505705, 0.4208383483546121, 0.4330081973757063, 0.424176721061979, 0.4111030467918941, 0.4070695834500449, 0.41967169897896905, 0.41810840112822395, 0.43753148913383483, 0.42339374933923996, 0.4624076707022531, 0.4114928986345019, 0.4405714503356389, 0.437255506004606]\n","Learning rate:\n","[0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 5e-05, 1e-06, 1e-06, 5e-05, 1e-06, 1e-06, 1e-06, 1e-06, 1e-06, 1e-06, 1e-06, 1e-06, 1e-06, 1e-06, 1e-06, 1e-06, 1e-06, 1e-06, 1e-06, 1e-06, 1e-06, 1e-06, 1e-06, 1e-06, 1e-06, 1e-06, 1e-06, 1e-06, 1e-06, 1e-06, 1e-06, 1e-06, 1e-06, 1e-06, 1e-06, 1e-06, 1e-06, 1e-06, 1e-06, 1e-06, 1e-06, 1e-06, 1e-06, 1e-06, 1e-06, 1e-06, 1e-06, 1e-06, 1e-06, 1e-06, 1e-06, 1e-06, 1e-06, 1e-06, 1e-06, 1e-06, 1e-06, 1e-06, 1e-06, 1e-06, 1e-06, 1e-06, 1e-06, 1e-06, 1e-06, 1e-06, 1e-06, 1e-06, 1e-06, 1e-06, 1e-06, 1e-06, 1e-06]\n","Epoch times:\n","[24.93934988975525, 46.184988021850586, 65.09458613395691, 83.9725432395935, 103.52129697799683, 122.57709956169128, 141.34096455574036, 160.85341382026672, 179.68637490272522, 198.75513434410095, 218.2753529548645, 237.4380431175232, 256.8338134288788, 275.95216274261475, 295.31844544410706, 314.5717840194702, 334.0437138080597, 353.7317819595337, 373.10996317863464, 392.46469020843506, 411.7889997959137, 430.57879853248596, 449.8521304130554, 469.41326332092285, 488.75253438949585, 508.7303669452667, 528.1400411128998, 547.2753002643585, 566.31134557724, 585.8258152008057, 605.1995825767517, 624.3960056304932, 643.7243750095367, 663.0614538192749, 681.9530625343323, 701.4608373641968, 720.5102877616882, 739.4146699905396, 759.0986359119415, 778.3230395317078, 797.1971576213837, 816.5139491558075, 835.2895061969757, 854.1621351242065, 873.2718033790588, 892.6041035652161, 911.5754687786102, 930.6486160755157, 950.4394245147705, 969.5147383213043, 988.6960413455963, 1008.0870680809021, 1026.9534537792206, 1045.8812379837036, 1065.6749420166016, 1084.4606354236603, 1103.4215314388275, 1122.8238260746002, 1141.9874305725098, 1161.0216944217682, 1180.1920447349548, 1199.3625066280365, 1218.7027904987335, 1238.8620109558105, 1258.0276846885681, 1276.8677451610565, 1296.286729335785, 1315.245491027832, 1334.231324672699, 1353.2959475517273, 1372.4890167713165, 1391.4444370269775, 1410.2994871139526, 1429.5636467933655, 1448.662972688675, 1467.6149756908417, 1486.9131078720093, 1505.881117105484, 1524.7814366817474, 1544.9555830955505, 1563.9592781066895, 1583.0421869754791, 1602.4123375415802, 1621.258736371994, 1640.3561363220215, 1659.4744741916656, 1678.7163503170013, 1697.9213571548462, 1717.3508701324463, 1737.0363173484802, 1755.9557871818542, 1775.3546953201294, 1794.7628064155579, 1813.5981795787811, 1832.9291071891785, 1852.5208480358124, 1871.654048204422, 1891.1140291690826, 1910.259385585785, 1929.384174823761]\n"]}],"source":["import torch\n","import torchvision.models as models\n","import torchvision.transforms as transforms\n","from torchvision import datasets\n","from torch.utils.data import DataLoader, random_split\n","import torch.nn as nn\n","import random\n","import time\n","\n","time0 = time.time()\n","\n","random.seed(10)\n","\n","train_acc = []\n","test_acc = []\n","loss_train = []\n","loss_test = []\n","epoch_time = []\n","learning_rates = []\n","\n","class RCCN7DataLoader:\n","    def __init__(self, data_dir, batch_size=32, shuffle=True):\n","        self.data_dir = data_dir\n","        self.batch_size = batch_size\n","        self.shuffle = shuffle\n","\n","        self.transform = transforms.Compose([\n","            transforms.Resize((256, 256)),\n","            transforms.ToTensor(),\n","            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","        ])\n","\n","        self.dataset = datasets.ImageFolder(root=self.data_dir, transform=self.transform)\n","        self.train_dataset, self.test_dataset = self.split_dataset()\n","\n","    def split_dataset(self):\n","        train_size = int(0.8 * len(self.dataset))\n","        test_size = len(self.dataset) - train_size\n","\n","        train_dataset, test_dataset = random_split(self.dataset, [train_size, test_size])\n","        return train_dataset, test_dataset\n","\n","    def get_train_dataloader(self):\n","        return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=self.shuffle)\n","\n","    def get_test_dataloader(self):\n","        return DataLoader(self.test_dataset, batch_size=self.batch_size, shuffle=False)\n","\n","data_dir = '/kaggle/input/rssnc7/RSSCN7'\n","batch_size = 32\n","learning_rate = 0.001\n","\n","data_loader = RCCN7DataLoader(data_dir=data_dir, batch_size=batch_size, shuffle=True)\n","\n","resnet18 = models.resnet18(weights='ResNet18_Weights.DEFAULT')\n","\n","\n","for name, param in resnet18.named_parameters():\n","    if 'fc' not in name:\n","        param.requires_grad = False\n","\n","num_classes = 7\n","\n","resnet18.fc = nn.Linear(resnet18.fc.in_features, num_classes)\n","\n","device = \"cuda\"\n","resnet18 = resnet18.to(device)\n","\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(resnet18.parameters(), lr=learning_rate)\n","\n","#most optimal\n","epochs = 100\n","\n","def train_model(model, criterion, optimizer, train_loader, test_loader, num_epochs, learning_rate):\n","    for epoch in range(num_epochs):\n","        model.train()\n","        running_loss = 0.0\n","        correct_predictions = 0\n","        total_samples = 0\n","\n","        for inputs, labels in train_loader:\n","            inputs, labels = inputs.to(device), labels.to(device)\n","\n","            optimizer.zero_grad()\n","\n","            outputs = model(inputs)\n","            loss = criterion(outputs, labels)\n","            loss.backward()\n","            optimizer.step()\n","\n","            running_loss += loss.item() * inputs.size(0)\n","            _, predicted = torch.max(outputs, 1)\n","            correct_predictions += (predicted == labels).sum().item()\n","            total_samples += labels.size(0)\n","\n","        epoch_loss = running_loss / total_samples\n","        epoch_accuracy = correct_predictions / total_samples\n","\n","        if epoch_accuracy >= 85:\n","            learning_rate = 0.0005\n","\n","        if epoch_accuracy >= 89:\n","            learning_rate = 0.00005\n","\n","        if epoch_accuracy >= 92:\n","            learning_rate = 0.000001\n","\n","        time_cur = time.time() - time0\n","        print(\"learing_rate = \", learning_rate)\n","\n","        print(f'Training - Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.4f}')\n","\n","        test_loss, test_accuracy = evaluate_model(model, criterion, test_loader)\n","        print(f'Testing - Epoch {epoch+1}/{num_epochs}, Loss: {test_loss:.4f}, Accuracy: {test_accuracy:.4f}')\n","        print(f'Time: {time_cur:.2f} seconds')\n","\n","        train_acc.append(epoch_accuracy)\n","        test_acc.append(test_accuracy)\n","        loss_train.append(epoch_loss)\n","        loss_test.append(test_loss)\n","        learning_rates.append(learning_rate)\n","        epoch_time.append(time_cur)\n","\n","    print('Training complete.')\n","\n","def evaluate_model(model, criterion, test_loader):\n","    model.eval()\n","    running_loss = 0.0\n","    correct_predictions = 0\n","    total_samples = 0\n","\n","    with torch.no_grad():\n","        for inputs, labels in test_loader:\n","            inputs, labels = inputs.to(device), labels.to(device)\n","\n","            outputs = model(inputs)\n","            loss = criterion(outputs, labels)\n","\n","            running_loss += loss.item() * inputs.size(0)\n","            _, predicted = torch.max(outputs, 1)\n","            correct_predictions += (predicted == labels).sum().item()\n","            total_samples += labels.size(0)\n","\n","    test_loss = running_loss / total_samples\n","    test_accuracy = correct_predictions / total_samples\n","\n","    return test_loss, test_accuracy\n","\n","train_loader = data_loader.get_train_dataloader()\n","test_loader = data_loader.get_test_dataloader()\n","\n","train_model(resnet18, criterion, optimizer, train_loader, test_loader, epochs, learning_rate)\n","\n","print('Training completed successfully.')\n","print(\"Training accuracy:\")\n","print(train_acc)\n","print(\"Test accuracy:\")\n","print(test_acc)\n","print('Loss train:')\n","print(loss_train)\n","print(\"Test loss:\")\n","print(loss_test)\n","print(\"Learning rate:\")\n","print(learning_rates)\n","print(\"Epoch times:\")\n","print(epoch_time)\n","\n","torch.save(resnet18, 'resnet18_imagenet_transfer_learning.pth')"]},{"cell_type":"raw","metadata":{},"source":["Transfer Learning DTD"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-05-28T09:25:27.621744Z","iopub.status.busy":"2024-05-28T09:25:27.621400Z","iopub.status.idle":"2024-05-28T09:57:30.491728Z","shell.execute_reply":"2024-05-28T09:57:30.490373Z","shell.execute_reply.started":"2024-05-28T09:25:27.621718Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n","  warnings.warn(msg)\n"]},{"name":"stdout","output_type":"stream","text":["learning_rate =  0.001\n","Training - Epoch 1/100, Loss: 4.0110, Accuracy: 0.2576\n","Testing - Epoch 1/100, Loss: 2.7163, Accuracy: 0.3161\n","Time: 29.39 seconds\n","learning_rate =  0.001\n","Training - Epoch 2/100, Loss: 1.8973, Accuracy: 0.4647\n","Testing - Epoch 2/100, Loss: 1.4172, Accuracy: 0.5214\n","Time: 51.02 seconds\n","learning_rate =  0.001\n","Training - Epoch 3/100, Loss: 1.2247, Accuracy: 0.6045\n","Testing - Epoch 3/100, Loss: 1.0610, Accuracy: 0.6411\n","Time: 70.00 seconds\n","learning_rate =  0.001\n","Training - Epoch 4/100, Loss: 0.9689, Accuracy: 0.6741\n","Testing - Epoch 4/100, Loss: 0.8938, Accuracy: 0.6964\n","Time: 89.01 seconds\n","learning_rate =  0.001\n","Training - Epoch 5/100, Loss: 0.8448, Accuracy: 0.7058\n","Testing - Epoch 5/100, Loss: 0.7807, Accuracy: 0.7304\n","Time: 107.73 seconds\n","learning_rate =  0.001\n","Training - Epoch 6/100, Loss: 0.7634, Accuracy: 0.7210\n","Testing - Epoch 6/100, Loss: 0.7291, Accuracy: 0.7321\n","Time: 126.31 seconds\n","learning_rate =  0.001\n","Training - Epoch 7/100, Loss: 0.7233, Accuracy: 0.7366\n","Testing - Epoch 7/100, Loss: 0.6633, Accuracy: 0.7446\n","Time: 145.27 seconds\n","learning_rate =  0.001\n","Training - Epoch 8/100, Loss: 0.6641, Accuracy: 0.7576\n","Testing - Epoch 8/100, Loss: 0.6462, Accuracy: 0.7643\n","Time: 164.15 seconds\n","learning_rate =  0.001\n","Training - Epoch 9/100, Loss: 0.6212, Accuracy: 0.7594\n","Testing - Epoch 9/100, Loss: 0.5912, Accuracy: 0.7875\n","Time: 182.87 seconds\n","learning_rate =  0.001\n","Training - Epoch 10/100, Loss: 0.5998, Accuracy: 0.7750\n","Testing - Epoch 10/100, Loss: 0.6027, Accuracy: 0.7804\n","Time: 202.22 seconds\n","learning_rate =  0.001\n","Training - Epoch 11/100, Loss: 0.5547, Accuracy: 0.7969\n","Testing - Epoch 11/100, Loss: 0.5687, Accuracy: 0.7857\n","Time: 220.85 seconds\n","learning_rate =  0.001\n","Training - Epoch 12/100, Loss: 0.5679, Accuracy: 0.8022\n","Testing - Epoch 12/100, Loss: 0.5733, Accuracy: 0.7839\n","Time: 239.69 seconds\n","learning_rate =  0.001\n","Training - Epoch 13/100, Loss: 0.5544, Accuracy: 0.7960\n","Testing - Epoch 13/100, Loss: 0.5493, Accuracy: 0.7893\n","Time: 258.71 seconds\n","learning_rate =  0.001\n","Training - Epoch 14/100, Loss: 0.5445, Accuracy: 0.8080\n","Testing - Epoch 14/100, Loss: 0.5357, Accuracy: 0.7946\n","Time: 277.85 seconds\n","learning_rate =  0.001\n","Training - Epoch 15/100, Loss: 0.5392, Accuracy: 0.8000\n","Testing - Epoch 15/100, Loss: 0.5180, Accuracy: 0.8161\n","Time: 296.53 seconds\n","learning_rate =  0.001\n","Training - Epoch 16/100, Loss: 0.5173, Accuracy: 0.8076\n","Testing - Epoch 16/100, Loss: 0.5502, Accuracy: 0.8036\n","Time: 315.36 seconds\n","learning_rate =  0.001\n","Training - Epoch 17/100, Loss: 0.5001, Accuracy: 0.8214\n","Testing - Epoch 17/100, Loss: 0.5440, Accuracy: 0.7964\n","Time: 334.30 seconds\n","learning_rate =  0.001\n","Training - Epoch 18/100, Loss: 0.4877, Accuracy: 0.8254\n","Testing - Epoch 18/100, Loss: 0.5084, Accuracy: 0.8089\n","Time: 353.10 seconds\n","learning_rate =  0.001\n","Training - Epoch 19/100, Loss: 0.4919, Accuracy: 0.8129\n","Testing - Epoch 19/100, Loss: 0.5094, Accuracy: 0.8071\n","Time: 372.12 seconds\n","learning_rate =  0.001\n","Training - Epoch 20/100, Loss: 0.4786, Accuracy: 0.8192\n","Testing - Epoch 20/100, Loss: 0.4875, Accuracy: 0.8286\n","Time: 391.26 seconds\n","learning_rate =  0.001\n","Training - Epoch 21/100, Loss: 0.4687, Accuracy: 0.8241\n","Testing - Epoch 21/100, Loss: 0.4834, Accuracy: 0.8232\n","Time: 409.96 seconds\n","learning_rate =  0.001\n","Training - Epoch 22/100, Loss: 0.4660, Accuracy: 0.8272\n","Testing - Epoch 22/100, Loss: 0.4817, Accuracy: 0.8250\n","Time: 428.90 seconds\n","learning_rate =  0.001\n","Training - Epoch 23/100, Loss: 0.4488, Accuracy: 0.8335\n","Testing - Epoch 23/100, Loss: 0.4793, Accuracy: 0.8304\n","Time: 447.96 seconds\n","learning_rate =  0.001\n","Training - Epoch 24/100, Loss: 0.4317, Accuracy: 0.8442\n","Testing - Epoch 24/100, Loss: 0.4944, Accuracy: 0.8250\n","Time: 466.70 seconds\n","learning_rate =  0.001\n","Training - Epoch 25/100, Loss: 0.4406, Accuracy: 0.8268\n","Testing - Epoch 25/100, Loss: 0.4970, Accuracy: 0.8339\n","Time: 485.41 seconds\n","learning_rate =  0.001\n","Training - Epoch 26/100, Loss: 0.4458, Accuracy: 0.8321\n","Testing - Epoch 26/100, Loss: 0.4509, Accuracy: 0.8357\n","Time: 504.27 seconds\n","learning_rate =  0.001\n","Training - Epoch 27/100, Loss: 0.4302, Accuracy: 0.8433\n","Testing - Epoch 27/100, Loss: 0.4656, Accuracy: 0.8286\n","Time: 523.00 seconds\n","learning_rate =  0.001\n","Training - Epoch 28/100, Loss: 0.4343, Accuracy: 0.8429\n","Testing - Epoch 28/100, Loss: 0.4612, Accuracy: 0.8357\n","Time: 541.75 seconds\n","learning_rate =  0.001\n","Training - Epoch 29/100, Loss: 0.4260, Accuracy: 0.8388\n","Testing - Epoch 29/100, Loss: 0.4772, Accuracy: 0.8286\n","Time: 560.73 seconds\n","learning_rate =  0.001\n","Training - Epoch 30/100, Loss: 0.4304, Accuracy: 0.8424\n","Testing - Epoch 30/100, Loss: 0.4435, Accuracy: 0.8375\n","Time: 579.47 seconds\n","learning_rate =  0.001\n","Training - Epoch 31/100, Loss: 0.4040, Accuracy: 0.8491\n","Testing - Epoch 31/100, Loss: 0.4678, Accuracy: 0.8250\n","Time: 598.01 seconds\n","learning_rate =  0.0001\n","Training - Epoch 32/100, Loss: 0.3972, Accuracy: 0.8585\n","Testing - Epoch 32/100, Loss: 0.4507, Accuracy: 0.8321\n","Time: 617.14 seconds\n","learning_rate =  0.0001\n","Training - Epoch 33/100, Loss: 0.4038, Accuracy: 0.8571\n","Testing - Epoch 33/100, Loss: 0.4562, Accuracy: 0.8321\n","Time: 636.50 seconds\n","learning_rate =  0.0001\n","Training - Epoch 34/100, Loss: 0.4037, Accuracy: 0.8518\n","Testing - Epoch 34/100, Loss: 0.4429, Accuracy: 0.8393\n","Time: 655.40 seconds\n","learning_rate =  0.0001\n","Training - Epoch 35/100, Loss: 0.4031, Accuracy: 0.8536\n","Testing - Epoch 35/100, Loss: 0.4427, Accuracy: 0.8339\n","Time: 674.44 seconds\n","learning_rate =  0.0001\n","Training - Epoch 36/100, Loss: 0.3970, Accuracy: 0.8509\n","Testing - Epoch 36/100, Loss: 0.4451, Accuracy: 0.8375\n","Time: 693.15 seconds\n","learning_rate =  0.0001\n","Training - Epoch 37/100, Loss: 0.3781, Accuracy: 0.8580\n","Testing - Epoch 37/100, Loss: 0.4864, Accuracy: 0.8357\n","Time: 712.04 seconds\n","learning_rate =  0.0001\n","Training - Epoch 38/100, Loss: 0.4116, Accuracy: 0.8509\n","Testing - Epoch 38/100, Loss: 0.4606, Accuracy: 0.8304\n","Time: 731.26 seconds\n","learning_rate =  0.0001\n","Training - Epoch 39/100, Loss: 0.3924, Accuracy: 0.8571\n","Testing - Epoch 39/100, Loss: 0.4258, Accuracy: 0.8464\n","Time: 750.01 seconds\n","learning_rate =  0.0001\n","Training - Epoch 40/100, Loss: 0.3759, Accuracy: 0.8683\n","Testing - Epoch 40/100, Loss: 0.4371, Accuracy: 0.8339\n","Time: 768.72 seconds\n","learning_rate =  0.0001\n","Training - Epoch 41/100, Loss: 0.3797, Accuracy: 0.8612\n","Testing - Epoch 41/100, Loss: 0.4333, Accuracy: 0.8393\n","Time: 787.30 seconds\n","learning_rate =  0.0001\n","Training - Epoch 42/100, Loss: 0.3591, Accuracy: 0.8679\n","Testing - Epoch 42/100, Loss: 0.4276, Accuracy: 0.8446\n","Time: 806.27 seconds\n","learning_rate =  0.0001\n","Training - Epoch 43/100, Loss: 0.3779, Accuracy: 0.8616\n","Testing - Epoch 43/100, Loss: 0.4560, Accuracy: 0.8411\n","Time: 825.11 seconds\n","learning_rate =  0.0001\n","Training - Epoch 44/100, Loss: 0.3728, Accuracy: 0.8643\n","Testing - Epoch 44/100, Loss: 0.4164, Accuracy: 0.8571\n","Time: 843.66 seconds\n","learning_rate =  0.0001\n","Training - Epoch 45/100, Loss: 0.3800, Accuracy: 0.8522\n","Testing - Epoch 45/100, Loss: 0.4351, Accuracy: 0.8304\n","Time: 862.90 seconds\n","learning_rate =  0.0001\n","Training - Epoch 46/100, Loss: 0.3645, Accuracy: 0.8638\n","Testing - Epoch 46/100, Loss: 0.4237, Accuracy: 0.8607\n","Time: 881.77 seconds\n","learning_rate =  0.0001\n","Training - Epoch 47/100, Loss: 0.3547, Accuracy: 0.8696\n","Testing - Epoch 47/100, Loss: 0.4340, Accuracy: 0.8339\n","Time: 900.60 seconds\n","learning_rate =  0.0001\n","Training - Epoch 48/100, Loss: 0.3683, Accuracy: 0.8621\n","Testing - Epoch 48/100, Loss: 0.4261, Accuracy: 0.8446\n","Time: 919.48 seconds\n","learning_rate =  0.0001\n","Training - Epoch 49/100, Loss: 0.3709, Accuracy: 0.8634\n","Testing - Epoch 49/100, Loss: 0.4142, Accuracy: 0.8446\n","Time: 938.88 seconds\n","learning_rate =  0.0001\n","Training - Epoch 50/100, Loss: 0.3381, Accuracy: 0.8902\n","Testing - Epoch 50/100, Loss: 0.4496, Accuracy: 0.8339\n","Time: 957.92 seconds\n","learning_rate =  0.0001\n","Training - Epoch 51/100, Loss: 0.3429, Accuracy: 0.8737\n","Testing - Epoch 51/100, Loss: 0.4171, Accuracy: 0.8411\n","Time: 977.33 seconds\n","learning_rate =  0.0001\n","Training - Epoch 52/100, Loss: 0.3747, Accuracy: 0.8576\n","Testing - Epoch 52/100, Loss: 0.4126, Accuracy: 0.8518\n","Time: 996.78 seconds\n","learning_rate =  0.0001\n","Training - Epoch 53/100, Loss: 0.3441, Accuracy: 0.8696\n","Testing - Epoch 53/100, Loss: 0.4415, Accuracy: 0.8357\n","Time: 1016.05 seconds\n","learning_rate =  0.0001\n","Training - Epoch 54/100, Loss: 0.3319, Accuracy: 0.8839\n","Testing - Epoch 54/100, Loss: 0.4250, Accuracy: 0.8536\n","Time: 1035.09 seconds\n","learning_rate =  0.0001\n","Training - Epoch 55/100, Loss: 0.3548, Accuracy: 0.8656\n","Testing - Epoch 55/100, Loss: 0.4412, Accuracy: 0.8411\n","Time: 1054.55 seconds\n","learning_rate =  0.0001\n","Training - Epoch 56/100, Loss: 0.3370, Accuracy: 0.8799\n","Testing - Epoch 56/100, Loss: 0.4241, Accuracy: 0.8518\n","Time: 1073.77 seconds\n","learning_rate =  0.0001\n","Training - Epoch 57/100, Loss: 0.3329, Accuracy: 0.8844\n","Testing - Epoch 57/100, Loss: 0.4132, Accuracy: 0.8500\n","Time: 1093.38 seconds\n","learning_rate =  0.0001\n","Training - Epoch 58/100, Loss: 0.3303, Accuracy: 0.8799\n","Testing - Epoch 58/100, Loss: 0.4226, Accuracy: 0.8500\n","Time: 1113.00 seconds\n","learning_rate =  0.0001\n","Training - Epoch 59/100, Loss: 0.3260, Accuracy: 0.8826\n","Testing - Epoch 59/100, Loss: 0.4323, Accuracy: 0.8357\n","Time: 1131.84 seconds\n","learning_rate =  0.0001\n","Training - Epoch 60/100, Loss: 0.3381, Accuracy: 0.8750\n","Testing - Epoch 60/100, Loss: 0.4077, Accuracy: 0.8482\n","Time: 1150.88 seconds\n","learning_rate =  0.0001\n","Training - Epoch 61/100, Loss: 0.3281, Accuracy: 0.8848\n","Testing - Epoch 61/100, Loss: 0.4225, Accuracy: 0.8464\n","Time: 1170.18 seconds\n","learning_rate =  0.0001\n","Training - Epoch 62/100, Loss: 0.3548, Accuracy: 0.8821\n","Testing - Epoch 62/100, Loss: 0.4022, Accuracy: 0.8536\n","Time: 1189.24 seconds\n","learning_rate =  0.0001\n","Training - Epoch 63/100, Loss: 0.3227, Accuracy: 0.8817\n","Testing - Epoch 63/100, Loss: 0.4341, Accuracy: 0.8446\n","Time: 1208.08 seconds\n","learning_rate =  0.0001\n","Training - Epoch 64/100, Loss: 0.3257, Accuracy: 0.8844\n","Testing - Epoch 64/100, Loss: 0.4166, Accuracy: 0.8482\n","Time: 1227.38 seconds\n","learning_rate =  0.0001\n","Training - Epoch 65/100, Loss: 0.3261, Accuracy: 0.8879\n","Testing - Epoch 65/100, Loss: 0.4021, Accuracy: 0.8536\n","Time: 1246.61 seconds\n","learning_rate =  0.0001\n","Training - Epoch 66/100, Loss: 0.3236, Accuracy: 0.8817\n","Testing - Epoch 66/100, Loss: 0.3972, Accuracy: 0.8500\n","Time: 1266.56 seconds\n","learning_rate =  0.0001\n","Training - Epoch 67/100, Loss: 0.3319, Accuracy: 0.8763\n","Testing - Epoch 67/100, Loss: 0.4150, Accuracy: 0.8536\n","Time: 1286.40 seconds\n","learning_rate =  0.0001\n","Training - Epoch 68/100, Loss: 0.3073, Accuracy: 0.8884\n","Testing - Epoch 68/100, Loss: 0.3966, Accuracy: 0.8571\n","Time: 1305.23 seconds\n","learning_rate =  0.0001\n","Training - Epoch 69/100, Loss: 0.3123, Accuracy: 0.8871\n","Testing - Epoch 69/100, Loss: 0.4087, Accuracy: 0.8500\n","Time: 1324.01 seconds\n","learning_rate =  0.0001\n","Training - Epoch 70/100, Loss: 0.3345, Accuracy: 0.8795\n","Testing - Epoch 70/100, Loss: 0.4116, Accuracy: 0.8518\n","Time: 1343.49 seconds\n","learning_rate =  0.0001\n","Training - Epoch 71/100, Loss: 0.3468, Accuracy: 0.8732\n","Testing - Epoch 71/100, Loss: 0.4189, Accuracy: 0.8446\n","Time: 1362.20 seconds\n","learning_rate =  0.0001\n","Training - Epoch 72/100, Loss: 0.3183, Accuracy: 0.8804\n","Testing - Epoch 72/100, Loss: 0.4283, Accuracy: 0.8500\n","Time: 1381.45 seconds\n","learning_rate =  0.0001\n","Training - Epoch 73/100, Loss: 0.3020, Accuracy: 0.8884\n","Testing - Epoch 73/100, Loss: 0.4259, Accuracy: 0.8446\n","Time: 1400.71 seconds\n","learning_rate =  0.0001\n","Training - Epoch 74/100, Loss: 0.3174, Accuracy: 0.8879\n","Testing - Epoch 74/100, Loss: 0.4250, Accuracy: 0.8357\n","Time: 1419.55 seconds\n","learning_rate =  0.0001\n","Training - Epoch 75/100, Loss: 0.3116, Accuracy: 0.8790\n","Testing - Epoch 75/100, Loss: 0.4284, Accuracy: 0.8518\n","Time: 1438.66 seconds\n","learning_rate =  0.0001\n","Training - Epoch 76/100, Loss: 0.3203, Accuracy: 0.8821\n","Testing - Epoch 76/100, Loss: 0.4057, Accuracy: 0.8554\n","Time: 1458.47 seconds\n","learning_rate =  0.0001\n","Training - Epoch 77/100, Loss: 0.3050, Accuracy: 0.8893\n","Testing - Epoch 77/100, Loss: 0.4668, Accuracy: 0.8268\n","Time: 1477.90 seconds\n","learning_rate =  0.0001\n","Training - Epoch 78/100, Loss: 0.3146, Accuracy: 0.8879\n","Testing - Epoch 78/100, Loss: 0.3910, Accuracy: 0.8589\n","Time: 1497.40 seconds\n","learning_rate =  0.0001\n","Training - Epoch 79/100, Loss: 0.3080, Accuracy: 0.8924\n","Testing - Epoch 79/100, Loss: 0.3987, Accuracy: 0.8518\n","Time: 1516.95 seconds\n","learning_rate =  0.0001\n","Training - Epoch 80/100, Loss: 0.3095, Accuracy: 0.8929\n","Testing - Epoch 80/100, Loss: 0.4356, Accuracy: 0.8357\n","Time: 1536.35 seconds\n","learning_rate =  0.0001\n","Training - Epoch 81/100, Loss: 0.2946, Accuracy: 0.9009\n","Testing - Epoch 81/100, Loss: 0.4265, Accuracy: 0.8429\n","Time: 1555.48 seconds\n","learning_rate =  0.0001\n","Training - Epoch 82/100, Loss: 0.2926, Accuracy: 0.8924\n","Testing - Epoch 82/100, Loss: 0.3865, Accuracy: 0.8607\n","Time: 1575.50 seconds\n","learning_rate =  0.0001\n","Training - Epoch 83/100, Loss: 0.2778, Accuracy: 0.9062\n","Testing - Epoch 83/100, Loss: 0.3860, Accuracy: 0.8571\n","Time: 1594.74 seconds\n","learning_rate =  0.0001\n","Training - Epoch 84/100, Loss: 0.3004, Accuracy: 0.8884\n","Testing - Epoch 84/100, Loss: 0.4302, Accuracy: 0.8429\n","Time: 1613.45 seconds\n","learning_rate =  0.0001\n","Training - Epoch 85/100, Loss: 0.3060, Accuracy: 0.8893\n","Testing - Epoch 85/100, Loss: 0.3945, Accuracy: 0.8625\n","Time: 1632.75 seconds\n","learning_rate =  0.0001\n","Training - Epoch 86/100, Loss: 0.2967, Accuracy: 0.8911\n","Testing - Epoch 86/100, Loss: 0.3876, Accuracy: 0.8589\n","Time: 1652.17 seconds\n","learning_rate =  0.0001\n","Training - Epoch 87/100, Loss: 0.2977, Accuracy: 0.8915\n","Testing - Epoch 87/100, Loss: 0.4049, Accuracy: 0.8571\n","Time: 1671.02 seconds\n","learning_rate =  0.0001\n","Training - Epoch 88/100, Loss: 0.2762, Accuracy: 0.9027\n","Testing - Epoch 88/100, Loss: 0.4465, Accuracy: 0.8339\n","Time: 1690.01 seconds\n","learning_rate =  0.0001\n","Training - Epoch 89/100, Loss: 0.3007, Accuracy: 0.8879\n","Testing - Epoch 89/100, Loss: 0.3921, Accuracy: 0.8625\n","Time: 1709.30 seconds\n","learning_rate =  0.0001\n","Training - Epoch 90/100, Loss: 0.2979, Accuracy: 0.8964\n","Testing - Epoch 90/100, Loss: 0.4161, Accuracy: 0.8518\n","Time: 1728.38 seconds\n","learning_rate =  0.0001\n","Training - Epoch 91/100, Loss: 0.2821, Accuracy: 0.8964\n","Testing - Epoch 91/100, Loss: 0.3864, Accuracy: 0.8643\n","Time: 1747.21 seconds\n","learning_rate =  0.0001\n","Training - Epoch 92/100, Loss: 0.2859, Accuracy: 0.8973\n","Testing - Epoch 92/100, Loss: 0.4142, Accuracy: 0.8500\n","Time: 1766.08 seconds\n","learning_rate =  0.0001\n","Training - Epoch 93/100, Loss: 0.3051, Accuracy: 0.8879\n","Testing - Epoch 93/100, Loss: 0.3908, Accuracy: 0.8625\n","Time: 1784.83 seconds\n","learning_rate =  0.0001\n","Training - Epoch 94/100, Loss: 0.3027, Accuracy: 0.8893\n","Testing - Epoch 94/100, Loss: 0.4013, Accuracy: 0.8607\n","Time: 1803.69 seconds\n","learning_rate =  0.0001\n","Training - Epoch 95/100, Loss: 0.2919, Accuracy: 0.8924\n","Testing - Epoch 95/100, Loss: 0.3987, Accuracy: 0.8571\n","Time: 1823.62 seconds\n","learning_rate =  0.0001\n","Training - Epoch 96/100, Loss: 0.3002, Accuracy: 0.8875\n","Testing - Epoch 96/100, Loss: 0.4256, Accuracy: 0.8357\n","Time: 1843.20 seconds\n","learning_rate =  0.0001\n","Training - Epoch 97/100, Loss: 0.2995, Accuracy: 0.8902\n","Testing - Epoch 97/100, Loss: 0.3976, Accuracy: 0.8500\n","Time: 1861.94 seconds\n","learning_rate =  0.0001\n","Training - Epoch 98/100, Loss: 0.2907, Accuracy: 0.8955\n","Testing - Epoch 98/100, Loss: 0.4093, Accuracy: 0.8589\n","Time: 1881.22 seconds\n","learning_rate =  0.0001\n","Training - Epoch 99/100, Loss: 0.2877, Accuracy: 0.8942\n","Testing - Epoch 99/100, Loss: 0.4309, Accuracy: 0.8500\n","Time: 1899.87 seconds\n","learning_rate =  0.0001\n","Training - Epoch 100/100, Loss: 0.2919, Accuracy: 0.8982\n","Testing - Epoch 100/100, Loss: 0.3953, Accuracy: 0.8571\n","Time: 1918.96 seconds\n","Training complete.\n","Training completed successfully.\n","Training accuracy:\n","[0.2575892857142857, 0.46473214285714287, 0.6044642857142857, 0.6741071428571429, 0.7058035714285714, 0.7209821428571429, 0.7366071428571429, 0.7575892857142857, 0.759375, 0.775, 0.796875, 0.8022321428571428, 0.7959821428571429, 0.8080357142857143, 0.8, 0.8075892857142857, 0.8214285714285714, 0.8254464285714286, 0.8129464285714286, 0.8191964285714286, 0.8241071428571428, 0.8272321428571429, 0.8334821428571428, 0.8441964285714286, 0.8267857142857142, 0.8321428571428572, 0.8433035714285714, 0.8428571428571429, 0.8388392857142857, 0.8424107142857142, 0.8491071428571428, 0.8584821428571429, 0.8571428571428571, 0.8517857142857143, 0.8535714285714285, 0.8508928571428571, 0.8580357142857142, 0.8508928571428571, 0.8571428571428571, 0.8683035714285714, 0.8611607142857143, 0.8678571428571429, 0.8616071428571429, 0.8642857142857143, 0.8522321428571429, 0.8638392857142857, 0.8696428571428572, 0.8620535714285714, 0.8633928571428572, 0.8901785714285714, 0.8736607142857142, 0.8575892857142857, 0.8696428571428572, 0.8839285714285714, 0.865625, 0.8799107142857143, 0.884375, 0.8799107142857143, 0.8825892857142857, 0.875, 0.8848214285714285, 0.8821428571428571, 0.8816964285714286, 0.884375, 0.8879464285714286, 0.8816964285714286, 0.8763392857142858, 0.8883928571428571, 0.8870535714285714, 0.8794642857142857, 0.8732142857142857, 0.8803571428571428, 0.8883928571428571, 0.8879464285714286, 0.8790178571428572, 0.8821428571428571, 0.8892857142857142, 0.8879464285714286, 0.8924107142857143, 0.8928571428571429, 0.9008928571428572, 0.8924107142857143, 0.90625, 0.8883928571428571, 0.8892857142857142, 0.8910714285714286, 0.8915178571428571, 0.9026785714285714, 0.8879464285714286, 0.8964285714285715, 0.8964285714285715, 0.8973214285714286, 0.8879464285714286, 0.8892857142857142, 0.8924107142857143, 0.8875, 0.8901785714285714, 0.8955357142857143, 0.8941964285714286, 0.8982142857142857]\n","Test accuracy:\n","[0.31607142857142856, 0.5214285714285715, 0.6410714285714286, 0.6964285714285714, 0.7303571428571428, 0.7321428571428571, 0.7446428571428572, 0.7642857142857142, 0.7875, 0.7803571428571429, 0.7857142857142857, 0.7839285714285714, 0.7892857142857143, 0.7946428571428571, 0.8160714285714286, 0.8035714285714286, 0.7964285714285714, 0.8089285714285714, 0.8071428571428572, 0.8285714285714286, 0.8232142857142857, 0.825, 0.8303571428571429, 0.825, 0.8339285714285715, 0.8357142857142857, 0.8285714285714286, 0.8357142857142857, 0.8285714285714286, 0.8375, 0.825, 0.8321428571428572, 0.8321428571428572, 0.8392857142857143, 0.8339285714285715, 0.8375, 0.8357142857142857, 0.8303571428571429, 0.8464285714285714, 0.8339285714285715, 0.8392857142857143, 0.8446428571428571, 0.8410714285714286, 0.8571428571428571, 0.8303571428571429, 0.8607142857142858, 0.8339285714285715, 0.8446428571428571, 0.8446428571428571, 0.8339285714285715, 0.8410714285714286, 0.8517857142857143, 0.8357142857142857, 0.8535714285714285, 0.8410714285714286, 0.8517857142857143, 0.85, 0.85, 0.8357142857142857, 0.8482142857142857, 0.8464285714285714, 0.8535714285714285, 0.8446428571428571, 0.8482142857142857, 0.8535714285714285, 0.85, 0.8535714285714285, 0.8571428571428571, 0.85, 0.8517857142857143, 0.8446428571428571, 0.85, 0.8446428571428571, 0.8357142857142857, 0.8517857142857143, 0.8553571428571428, 0.8267857142857142, 0.8589285714285714, 0.8517857142857143, 0.8357142857142857, 0.8428571428571429, 0.8607142857142858, 0.8571428571428571, 0.8428571428571429, 0.8625, 0.8589285714285714, 0.8571428571428571, 0.8339285714285715, 0.8625, 0.8517857142857143, 0.8642857142857143, 0.85, 0.8625, 0.8607142857142858, 0.8571428571428571, 0.8357142857142857, 0.85, 0.8589285714285714, 0.85, 0.8571428571428571]\n","Loss train:\n","[4.011032564299447, 1.8972771303994316, 1.2247104883193969, 0.9688620154346739, 0.84475593992642, 0.7633776771170753, 0.7232721086059298, 0.6641196898051671, 0.6211547911167145, 0.599787311894553, 0.5546612946050508, 0.5679011410900525, 0.554440872158323, 0.5444797979933875, 0.5392136309828077, 0.5173193850687572, 0.5001129254698753, 0.4876774719783238, 0.49186709352902, 0.4785576501062938, 0.468677439221314, 0.46603460418326514, 0.4488352903297969, 0.43167298776762825, 0.4405889700566019, 0.44579527186495915, 0.4301806324294635, 0.4342570021748543, 0.42601996958255767, 0.43043874991791586, 0.40396194798605783, 0.39719714841672354, 0.4038448521069118, 0.4037257916160992, 0.4030918395945004, 0.39704827815294264, 0.3780681303569249, 0.41156849073512214, 0.39239185197012766, 0.3758822873234749, 0.3797275430389813, 0.35906217311109817, 0.37786441019603184, 0.3727979293891362, 0.3800413978951318, 0.3644957071968487, 0.3546755015850067, 0.36832023837736677, 0.3709263720682689, 0.3381390741893223, 0.342908761543887, 0.37471544103963034, 0.3441260644367763, 0.3319265812635422, 0.35480674164635795, 0.3369794038789613, 0.33285661382334575, 0.3302720797913415, 0.3260045699775219, 0.3381365750517164, 0.32810446258102144, 0.35484942964145116, 0.32268465917025296, 0.32565766658101764, 0.32607218112264363, 0.3236482262611389, 0.33191952726670676, 0.30734745274697034, 0.31227819259677614, 0.3345008345586913, 0.3467956330095019, 0.318305787976299, 0.3019514228616442, 0.3173666319676808, 0.31164876094886235, 0.3202808380126953, 0.3050488643348217, 0.31457219038690837, 0.30796347579785754, 0.30954159285340993, 0.2945851304701396, 0.2926340131887368, 0.2777624615601131, 0.3004159892243998, 0.3060461370008332, 0.29674555893455234, 0.2977292307785579, 0.27623230761715345, 0.3007275026823793, 0.29786353441221375, 0.2820679145199912, 0.2858625811125551, 0.3051322493169989, 0.30274876419986996, 0.2919483085828168, 0.30022701269813945, 0.29950442569596425, 0.29070863191570556, 0.2876711352595261, 0.29191910411630356]\n","Test loss:\n","[2.7163328613553728, 1.4172410113470895, 1.0610152091298783, 0.8937725143773215, 0.7807373234203884, 0.7291359501225608, 0.6633276564734323, 0.6461966259138925, 0.591193254504885, 0.6026858525616782, 0.5686950990131923, 0.5732602460043771, 0.5493181024278914, 0.5357465369360788, 0.5179714288030351, 0.5502198926040105, 0.5440050414630345, 0.5083716877869198, 0.5094372970717294, 0.48752514975411554, 0.483368832724435, 0.4817440969603402, 0.4793070444038936, 0.49443304538726807, 0.49695766823632376, 0.45086956449917387, 0.46555825982775006, 0.4612330496311188, 0.47722708327429636, 0.44347339272499087, 0.4678123959473201, 0.45068034529685974, 0.4561645124639784, 0.4429003485611507, 0.44273159503936765, 0.4450558934892927, 0.4863553787980761, 0.4605599616255079, 0.42575898425919667, 0.43711539677211214, 0.43330001320157735, 0.4276367119380406, 0.45601119824818204, 0.416378573008946, 0.4351194032600948, 0.42368806515421187, 0.4339899914605277, 0.4260919460228511, 0.41418932165418354, 0.4495989424841745, 0.4171206942626408, 0.412625903742654, 0.44145078063011167, 0.42495072313717436, 0.4412474538598742, 0.42411878534725733, 0.41318929025105067, 0.42262059194701057, 0.43226068105016435, 0.407677287714822, 0.42250243510518753, 0.4021664023399353, 0.43413685049329487, 0.4166330754756927, 0.40209019609860014, 0.3972212893622262, 0.4149985151631492, 0.39663170661245073, 0.40871763484818596, 0.41164165139198305, 0.4189331122807094, 0.4283494617257799, 0.4258740041937147, 0.4249570071697235, 0.4284449790205274, 0.40573673844337466, 0.46682732360703605, 0.3910374011312212, 0.3986620937074934, 0.4356306195259094, 0.4264868353094373, 0.3864997020789555, 0.38596827558108737, 0.43021456599235536, 0.39449027265821185, 0.3875631323882512, 0.4049052442823138, 0.44653433561325073, 0.3920751988887787, 0.41607847043446133, 0.3863918440682547, 0.41421360543795993, 0.3908379307815007, 0.40130760840007235, 0.39873209084783284, 0.4256242300782885, 0.3975824304989406, 0.409307952438082, 0.43086038742746624, 0.3952910670212337]\n","Learning rate:\n","[0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001]\n","Epoch times:\n","[29.38589334487915, 51.02310037612915, 70.00154900550842, 89.00890779495239, 107.72826838493347, 126.30855059623718, 145.2695438861847, 164.14990139007568, 182.86766648292542, 202.22443318367004, 220.84687733650208, 239.68839073181152, 258.71440386772156, 277.8530411720276, 296.5302953720093, 315.35954236984253, 334.2983832359314, 353.10147047042847, 372.1151294708252, 391.26002621650696, 409.9569661617279, 428.8986222743988, 447.95583057403564, 466.7014513015747, 485.414186000824, 504.2747347354889, 522.9953043460846, 541.7482280731201, 560.7337353229523, 579.4676959514618, 598.0123827457428, 617.1365482807159, 636.5009214878082, 655.4035618305206, 674.4370219707489, 693.1485214233398, 712.0435781478882, 731.2571244239807, 750.0139138698578, 768.7243809700012, 787.3004634380341, 806.2709422111511, 825.1053166389465, 843.6644546985626, 862.9031112194061, 881.7678878307343, 900.5978105068207, 919.4772431850433, 938.8790109157562, 957.9193794727325, 977.3296916484833, 996.7811436653137, 1016.0513632297516, 1035.0887570381165, 1054.5513544082642, 1073.7703957557678, 1093.3763287067413, 1112.997496843338, 1131.837991476059, 1150.8795988559723, 1170.1759564876556, 1189.236900806427, 1208.0770337581635, 1227.3801851272583, 1246.607885837555, 1266.5570640563965, 1286.3977863788605, 1305.2303647994995, 1324.0102574825287, 1343.4930293560028, 1362.1977579593658, 1381.4468786716461, 1400.7072281837463, 1419.5454769134521, 1438.6594908237457, 1458.4698631763458, 1477.9005992412567, 1497.403294801712, 1516.9542922973633, 1536.3527135849, 1555.4815683364868, 1575.5040273666382, 1594.7415251731873, 1613.4468922615051, 1632.747376203537, 1652.1706750392914, 1671.0182573795319, 1690.014161348343, 1709.3043117523193, 1728.378984451294, 1747.2053117752075, 1766.0804505348206, 1784.830489397049, 1803.693995475769, 1823.622707605362, 1843.2018830776215, 1861.9404909610748, 1881.2201447486877, 1899.8697521686554, 1918.961169242859]\n"]}],"source":["import torch\n","import torch.nn as nn\n","import random\n","import time\n","import torchvision.transforms as transforms\n","from torch.utils.data import DataLoader, random_split\n","from torchvision import datasets\n","\n","class RSSCN7_DataLoader:\n","    def __init__(self, data_dir, batch_size=32, shuffle=False):\n","        self.data_dir = data_dir\n","        self.batch_size = batch_size\n","        self.shuffle = shuffle\n","\n","        self.transform = transforms.Compose([\n","            transforms.Resize((256, 256)),\n","            transforms.ToTensor(),\n","            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","        ])\n","\n","        self.dataset = datasets.ImageFolder(root=self.data_dir, transform=self.transform)\n","        self.train_dataset, self.test_dataset = self.split_dataset()\n","\n","    def split_dataset(self):\n","        train_size = int(0.8 * len(self.dataset))\n","        test_size = len(self.dataset) - train_size\n","\n","        train_dataset, test_dataset = random_split(self.dataset, [train_size, test_size])\n","        return train_dataset, test_dataset\n","\n","    def get_train_dataloader(self):\n","        return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=self.shuffle)\n","\n","    def get_test_dataloader(self):\n","        return DataLoader(self.test_dataset, batch_size=self.batch_size, shuffle=False)\n","\n","time0 = time.time()\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","random.seed(10)\n","\n","train_acc = []\n","test_acc = []\n","loss_train = []\n","loss_test = []\n","epoch_time = []\n","learning_rates = []\n","\n","data_dir = '/kaggle/input/rssnc7/RSSCN7'\n","batch_size = 32\n","learning_rate = 0.001\n","\n","data_loader = RSSCN7_DataLoader(data_dir=data_dir, batch_size=batch_size, shuffle=True)\n","\n","pretrained_model_path = '/kaggle/input/resnet18-pretrained-on-dtd/pytorch/version1/1/resnet18_trained_on_DTD_from_80_to_90.pth'\n","pretrained_resnet18 = ResNet18()\n","pretrained_resnet18.load_state_dict(torch.load(pretrained_model_path, map_location=device))\n","\n","pretrained_resnet18.fc = nn.Linear(pretrained_resnet18.resnet18.fc.in_features, 7)\n","\n","pretrained_resnet18 = pretrained_resnet18.to(device)\n","\n","for name, param in pretrained_resnet18.named_parameters():\n","    if 'fc' not in name:\n","        param.requires_grad = False\n","\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(pretrained_resnet18.parameters(), lr=learning_rate)\n","\n","# most optimal\n","epochs = 100\n","\n","def train_model(model, criterion, optimizer, train_loader, test_loader, num_epochs, learning_rate):\n","    for epoch in range(num_epochs):\n","        model.train()\n","        running_loss = 0.0\n","        correct_predictions = 0\n","        total_samples = 0\n","\n","        for inputs, labels in train_loader:\n","            inputs, labels = inputs.to(device), labels.to(device)\n","\n","            optimizer.zero_grad()\n","\n","            outputs = model(inputs)\n","            loss = criterion(outputs, labels)\n","            loss.backward()\n","            optimizer.step()\n","\n","            running_loss += loss.item() * inputs.size(0)\n","            _, predicted = torch.max(outputs, 1)\n","            correct_predictions += (predicted == labels).sum().item()\n","            total_samples += labels.size(0)\n","\n","        epoch_loss = running_loss / total_samples\n","        epoch_accuracy = correct_predictions / total_samples \n","\n","        if epoch_accuracy >= 0.85:\n","            learning_rate = 0.0001\n","\n","\n","        time_cur = time.time() - time0\n","        print(\"learning_rate = \", learning_rate)\n","\n","        print(f'Training - Epoch {epoch + 1}/{num_epochs}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.4f}')\n","\n","        test_loss, test_accuracy = evaluate_model(model, criterion, test_loader)\n","        print(f'Testing - Epoch {epoch + 1}/{num_epochs}, Loss: {test_loss:.4f}, Accuracy: {test_accuracy:.4f}')\n","        print(f'Time: {time_cur:.2f} seconds')\n","\n","        train_acc.append(epoch_accuracy)\n","        test_acc.append(test_accuracy)\n","        loss_train.append(epoch_loss)\n","        loss_test.append(test_loss)\n","        learning_rates.append(learning_rate)\n","        epoch_time.append(time_cur)\n","\n","    print('Training complete.')\n","\n","def evaluate_model(model, criterion, test_loader):\n","    model.eval()\n","    running_loss = 0.0\n","    correct_predictions = 0\n","    total_samples = 0\n","\n","    with torch.no_grad():\n","        for inputs, labels in test_loader:\n","            inputs, labels = inputs.to(device), labels.to(device)\n","\n","            outputs = model(inputs)\n","            loss = criterion(outputs, labels)\n","\n","            running_loss += loss.item() * inputs.size(0)\n","            _, predicted = torch.max(outputs, 1)\n","            correct_predictions += (predicted == labels).sum().item()\n","            total_samples += labels.size(0)\n","\n","    test_loss = running_loss / total_samples\n","    test_accuracy = correct_predictions / total_samples\n","\n","    return test_loss, test_accuracy\n","\n","train_loader = data_loader.get_train_dataloader()\n","test_loader = data_loader.get_test_dataloader()\n","\n","train_model(pretrained_resnet18, criterion, optimizer, train_loader, test_loader, epochs, learning_rate)\n","\n","print('Training completed successfully.')\n","print(\"Training accuracy:\")\n","print(train_acc)\n","print(\"Test accuracy:\")\n","print(test_acc)\n","print('Loss train:')\n","print(loss_train)\n","print(\"Test loss:\")\n","print(loss_test)\n","print(\"Learning rate:\")\n","print(learning_rates)\n","print(\"Epoch times:\")\n","print(epoch_time)\n","\n","torch.save(pretrained_resnet18, 'resnet18_DTD_transfer_learning.pth')\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":5044311,"sourceId":8461855,"sourceType":"datasetVersion"},{"modelInstanceId":45451,"sourceId":54224,"sourceType":"modelInstanceVersion"}],"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
